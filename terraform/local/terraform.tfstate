{
  "version": 4,
  "terraform_version": "1.2.7",
  "serial": 8,
  "lineage": "5348bb3e-faf5-4649-264f-550df43b8b87",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "kubernetes_service",
      "name": "druid",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": []
    },
    {
      "mode": "data",
      "type": "kubernetes_service",
      "name": "kafka",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": []
    },
    {
      "mode": "data",
      "type": "kubernetes_service",
      "name": "zookeeper",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": []
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "druid_cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "../druid-cluster",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "druid-cluster",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.21.1",
                "chart": "druid-cluster",
                "name": "druid-cluster",
                "namespace": "druid-raw",
                "revision": 1,
                "values": "{\"dp_vault_pgdb_admin_password\":\"S@nk2022\",\"druid.startup.logging.logProperties\":true,\"druid.storage.storageDirectory\":\"~/z/\",\"druid_broker_http_numConnections\":5,\"druid_broker_max_direct_size\":\"2G\",\"druid_broker_max_heap_size\":\"128M\",\"druid_broker_min_heap_size\":\"128M\",\"druid_broker_port\":8082,\"druid_broker_processing_buffer_sizeBytes\":\"256MiB\",\"druid_broker_processing_numThreads\":2,\"druid_broker_replicas\":1,\"druid_broker_service\":\"druid/broker\",\"druid_cache_type\":\"caffeine\",\"druid_cluster_type\":\"raw\",\"druid_coordinator_balancer_strategy\":\"diskNormalized\",\"druid_coordinator_heap_size\":\"128M\",\"druid_coordinator_period\":\"PT30S\",\"druid_coordinator_port\":8081,\"druid_coordinator_replicas\":1,\"druid_coordinator_service\":\"druid/coordinator\",\"druid_coordinator_startDelay\":\"PT30S\",\"druid_deepstorage_type\":\"local\",\"druid_directory\":\"/opt/druid\",\"druid_emitter\":\"composing\",\"druid_emitter_composing_emitters\":\"\\\"logging\\\"\",\"druid_emitter_logging_logLevel\":\"INFO\",\"druid_env\":\"dev\",\"druid_extensions_loadList\":\"\\\"druid-azure-extensions\\\", \\\"postgresql-metadata-storage\\\", \\\"druid-kafka-indexing-service\\\"\",\"druid_historical_cache_expiry\":3600000,\"druid_historical_cache_populateCache\":true,\"druid_historical_cache_size\":\"256MiB\",\"druid_historical_cache_unCacheable\":\"\\\"select\\\", \\\"scan\\\"\",\"druid_historical_cache_useCache\":false,\"druid_historical_max_direct_size\":\"1G\",\"druid_historical_max_heap_size\":\"256M\",\"druid_historical_min_heap_size\":\"256M\",\"druid_historical_persistent_volume_size\":\"1G\",\"druid_historical_port\":8084,\"druid_historical_processing_buffer_sizeBytes\":\"128MiB\",\"druid_historical_processing_numMergeBuffers\":2,\"druid_historical_processing_numThreads\":2,\"druid_historical_replicas\":1,\"druid_historical_service\":\"druid/historical\",\"druid_image\":\"apache/druid:0.21.1\",\"druid_indexer_fork_property_druid_processing_buffer_sizeBytes\":\"25MiB\",\"druid_indexer_fork_property_druid_processing_numThreads\":2,\"druid_indexer_fork_property_druid_server_http_numThreads\":25,\"druid_indexer_logs_prefix\":\"druid-task-logs\",\"druid_indexer_logs_type\":\"file\",\"druid_indexer_queue_startDelay\":\"PT30S\",\"druid_indexer_runner_javaOpts\":\"-server -Xms512m -Xmx512m -XX:+UseG1GC -XX:MaxGCPauseMillis=100\",\"druid_indexer_runner_type\":\"remote\",\"druid_indexer_storage_type\":\"metadata\",\"druid_indexer_task_baseTaskDir\":\"/var/task\",\"druid_indexer_task_restoreTasksOnRestart\":true,\"druid_indexer_tasklock_forceTimeChunkLock\":false,\"druid_indexing_doubleStorage\":\"double\",\"druid_javascript.enabled\":true,\"druid_javascript_enabled\":true,\"druid_metadata_storage_connector_connectURI\":\"jdbc:postgresql://postgres-postgresql.postgres/druiddb\",\"druid_metadata_storage_connector_password\":\"druid\",\"druid_metadata_storage_connector_user\":\"druid\",\"druid_metadata_storage_type\":\"postgresql\",\"druid_middlemanager_heap_size\":\"128M\",\"druid_middlemanager_peon_heap_size\":\"256M\",\"druid_middlemanager_persistent_volume_size\":\"4G\",\"druid_middlemanager_port\":8091,\"druid_middlemanager_replicas\":1,\"druid_middlemanager_service\":\"druid/middlemanager\",\"druid_monitoring\":false,\"druid_monitoring_monitors\":\"\\\"com.metamx.metrics.JvmMonitor\\\",\\\"org.apache.druid.java.util.metrics.JvmMonitor\\\"\",\"druid_namespace\":\"druid-raw\",\"druid_overlord_heap_size\":\"256M\",\"druid_overlord_port\":8090,\"druid_overlord_replicas\":1,\"druid_overlord_service\":\"druid/overlord\",\"druid_query_groupBy_maxMergingDictionarySize\":100000000,\"druid_query_groupBy_maxOnDiskStorage\":1073741824,\"druid_query_ondiskstorage_enabled\":true,\"druid_request_logging_dir\":\"/var/log/druid/\",\"druid_request_logging_type\":\"composing\",\"druid_router_coordinatorServiceName\":\"druid/coordinator\",\"druid_router_defaultBrokerServiceName\":\"druid/broker\",\"druid_router_heap_size\":\"256M\",\"druid_router_http_numConnections\":50,\"druid_router_http_numMaxThreads\":100,\"druid_router_http_readTimeout\":\"PT5M\",\"druid_router_managementProxy_enabled\":true,\"druid_router_plaintextPort\":8888,\"druid_router_replicas\":1,\"druid_router_service\":\"druid/router\",\"druid_segmentCache_locations\":\"{\\\"path\\\": \\\"/var/segments/store0\\\", \\\"maxSize\\\": 2000000000}, {\\\"path\\\": \\\"/var/segments/store1\\\", \\\"maxSize\\\": 2000000000}, {\\\"path\\\": \\\"/var/segments/store2\\\", \\\"maxSize\\\": 2000000000}, {\\\"path\\\": \\\"/var/segments/store3\\\", \\\"maxSize\\\": 2000000000}, {\\\"path\\\": \\\"/var/segments/store4\\\", \\\"maxSize\\\": 2000000000}\",\"druid_segmentCache_numLoadingThreads\":4,\"druid_selectors_coordinator_serviceName\":\"druid/coordinator\",\"druid_selectors_indexing_serviceName\":\"druid/overlord\",\"druid_server_http_numThreads\":100,\"druid_sql_enable\":true,\"druid_worker_capacity\":\"3\",\"mount_path\":\"/druid/data\",\"storageClass\":\"standard\",\"zookeeper\":{\"allowAnonymousLogin\":true,\"autopurge\":{\"purgeInterval\":0,\"snapRetainCount\":3},\"fourlwCommandsWhitelist\":\"srvr, mntr, ruok\",\"heapSize\":256,\"image\":{\"pullPolicy\":\"Always\",\"registry\":\"docker.io\",\"repository\":\"bitnami/zookeeper\",\"tag\":\"3.6-debian-10\"},\"initLimit\":10,\"listenOnAllIPs\":false,\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"probeCommandTimeout\":2,\"successThreshold\":1,\"timeoutSeconds\":5},\"maxClientCnxns\":60,\"maxSessionTimeout\":40000,\"minServerId\":1,\"preAllocSize\":65536,\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"probeCommandTimeout\":2,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":1,\"resources\":{\"requests\":{\"cpu\":\"256m\",\"memory\":\"256Mi\"}},\"securityContext\":{\"enabled\":true,\"fsGroup\":1001,\"runAsUser\":1001},\"service\":{\"annotations\":{},\"disableBaseClientPort\":false,\"electionPort\":3888,\"followerPort\":2888,\"headless\":{\"annotations\":{}},\"nodePorts\":{\"client\":\"\",\"clientTls\":\"\"},\"port\":2181,\"publishNotReadyAddresses\":true,\"tlsClientPort\":3181,\"type\":\"ClusterIP\"},\"snapCount\":100000,\"syncLimit\":5,\"tickTime\":2000}}",
                "version": "0.1.0"
              }
            ],
            "name": "druid-cluster",
            "namespace": "druid-raw",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 3000,
            "values": [
              "druid_env: \"dev\"\ndruid_cluster_type: \"raw\"\ndruid_image: \"apache/druid:0.21.1\"\ndruid_namespace: \"druid-raw\"\ndp_vault_pgdb_admin_password: S@nk2022\ndruid_monitoring: False\nmount_path: /druid/data\nstorageClass: standard\n\n######################### Druid common variables ########################\ndruid_directory: \"/opt/druid\"\ndruid_extensions_loadList: '\"druid-azure-extensions\", \"postgresql-metadata-storage\", \"druid-kafka-indexing-service\"'\n\n# Logging\n# Log all runtime properties on startup. Disable to avoid logging properties on startup:\ndruid.startup.logging.logProperties: true\n\n# Druid Metadata Store\ndruid_metadata_storage_type: \"postgresql\"\ndruid_metadata_storage_connector_connectURI: \"jdbc:postgresql://postgres-postgresql.postgres/druiddb\"\ndruid_metadata_storage_connector_user: \"druid\"\ndruid_metadata_storage_connector_password: \"druid\"\n\n# Druid Storage Type\ndruid_deepstorage_type: \"local\"\n#azure_deepstorage_container: \"\" \n#azure_storage_account: \"\"\n#azure_storage_key: \"\"\n#azure_storage_container: \"\"\ndruid.storage.storageDirectory: \"~/z/\"\n# Indexing service logs\n# For local disk (only viable in a cluster_type if this is a network mount):\ndruid_indexer_logs_type: \"file\"\n#druid_indexer_logs_container: \"\"\ndruid_indexer_logs_prefix: \"druid-task-logs\"\n\n# Service discovery\ndruid_selectors_indexing_serviceName: druid/overlord\ndruid_selectors_coordinator_serviceName: druid/coordinator\n\n# Monitoring\ndruid_monitoring_monitors: '\"com.metamx.metrics.JvmMonitor\",\"org.apache.druid.java.util.metrics.JvmMonitor\"'\ndruid_emitter: composing\ndruid_emitter_composing_emitters: '\"logging\"'\ndruid_emitter_logging_logLevel: INFO\n\n\n# Storage type of double columns\n# ommiting this will lead to index double as float at the storage layer\ndruid_indexing_doubleStorage: double\n\n#Writing query logs into file \ndruid_request_logging_type: composing\ndruid_request_logging_dir: \"/var/log/druid/\"\n\ndruid_javascript_enabled: true\ndruid_sql_enable: true\n\n####################### Druid Broker Variables ##########################\n\ndruid_broker_service: druid/broker\ndruid_broker_port: 8082\ndruid_broker_min_heap_size: 128M\ndruid_broker_max_heap_size: 128M\ndruid_broker_max_direct_size: 2G\n\n# HTTP server threads\ndruid_broker_http_numConnections: 5\ndruid_server_http_numThreads: 25\n\n# Processing threads and buffers\ndruid_broker_processing_buffer_sizeBytes: 256MiB\ndruid_broker_processing_numThreads: 2\n\ndruid_javascript.enabled: true\ndruid_sql_enable: true\n\ndruid_broker_replicas: 1\n\n##################### Druid Coordinator Variables #######################\n\ndruid_coordinator_service: druid/coordinator\ndruid_coordinator_port: 8081\ndruid_coordinator_heap_size: 128M\n\ndruid_coordinator_startDelay: PT30S\ndruid_coordinator_period: PT30S\ndruid_coordinator_balancer_strategy: diskNormalized\n\ndruid_coordinator_replicas: 1\n\n####################### Druid Overlord Variables ########################\n\ndruid_overlord_service: druid/overlord\ndruid_overlord_port: 8090\ndruid_overlord_heap_size: 256M\n\ndruid_indexer_queue_startDelay: PT30S\n\ndruid_indexer_runner_type: remote\ndruid_indexer_storage_type: metadata\n\n# Additional parameters for minor compaction\ndruid_indexer_tasklock_forceTimeChunkLock: false\n\ndruid_overlord_replicas: 1\n\n###################### Druid Historical Variables #######################\n\ndruid_historical_service: druid/historical\ndruid_historical_port: 8084\ndruid_historical_min_heap_size: 256M\ndruid_historical_max_heap_size: 256M\ndruid_historical_max_direct_size: 1G\n\n# HTTP server threads\ndruid_server_http_numThreads: 25\n\n# Processing threads and buffers\ndruid_historical_processing_buffer_sizeBytes: 128MiB\ndruid_historical_processing_numThreads: 2\ndruid_historical_processing_numMergeBuffers: 2\n\ndruid_query_ondiskstorage_enabled: True\ndruid_query_groupBy_maxMergingDictionarySize: 100000000\ndruid_query_groupBy_maxOnDiskStorage: 1073741824\n\n# Segmentstorage\ndruid_segmentCache_locations: '{\"path\": \"/var/segments/store0\", \"maxSize\": 2000000000}, {\"path\": \"/var/segments/store1\", \"maxSize\": 2000000000}, {\"path\": \"/var/segments/store2\", \"maxSize\": 2000000000}, {\"path\": \"/var/segments/store3\", \"maxSize\": 2000000000}, {\"path\": \"/var/segments/store4\", \"maxSize\": 2000000000}'\n\ndruid_historical_persistent_volume_size: 1G\n\ndruid_segmentCache_numLoadingThreads: 4\n\n# Caching\ndruid_historical_cache_useCache: False \ndruid_historical_cache_populateCache: true\ndruid_historical_cache_unCacheable: '\"select\", \"scan\"'\ndruid_cache_type: caffeine\ndruid_historical_cache_size: 256MiB\ndruid_historical_cache_expiry: 3600000\n\ndruid_historical_replicas: 1\n\n#################### Druid Middlemanager Variables ######################\n\ndruid_middlemanager_service: druid/middlemanager\ndruid_middlemanager_port: 8091\ndruid_middlemanager_heap_size: 128M\n\ndruid_middlemanager_persistent_volume_size: 4G\n\n# Number of tasks per middleManager\ndruid_worker_capacity: \"3\"\ndruid_middlemanager_peon_heap_size: 256M\n\n# Task launch parameters\ndruid_indexer_runner_javaOpts: \"-server -Xms512m -Xmx512m -XX:+UseG1GC -XX:MaxGCPauseMillis=100\"\ndruid_indexer_task_baseTaskDir: \"/var/task\"\n\n# Peon properties\ndruid_indexer_fork_property_druid_processing_buffer_sizeBytes: 25MiB\ndruid_indexer_fork_property_druid_processing_numThreads: 2\ndruid_indexer_fork_property_druid_server_http_numThreads: 25\n\n#Additional Parameters\ndruid_indexer_task_restoreTasksOnRestart: true\n\ndruid_middlemanager_replicas: 1\n\n######################## Druid Router Variables #########################\n\ndruid_router_service: druid/router\ndruid_router_plaintextPort: 8888\ndruid_router_heap_size: 256M\n\n# HTTP proxy\ndruid_router_http_numConnections: 50\ndruid_router_http_readTimeout: PT5M\ndruid_router_http_numMaxThreads: 100\ndruid_server_http_numThreads: 100\n\n# Service discovery\ndruid_router_defaultBrokerServiceName: druid/broker\ndruid_router_coordinatorServiceName: druid/coordinator\n\n# Management proxy to coordinator / overlord: required for unified web console.\ndruid_router_managementProxy_enabled: True\n\ndruid_router_replicas: 1\n\n#################### Zookeeper Variables ######################\n\nzookeeper:\n  image:\n    registry: docker.io\n    repository: bitnami/zookeeper\n    tag: 3.6-debian-10\n    pullPolicy: Always\n\n  heapSize: 256\n  replicaCount: 1\n\n  resources:\n    requests:\n      memory: 256Mi\n      cpu: 256m\n\n  tickTime: 2000\n  initLimit: 10\n  syncLimit: 5\n  preAllocSize: 65536\n  snapCount: 100000\n  maxClientCnxns: 60\n  fourlwCommandsWhitelist: srvr, mntr, ruok\n  listenOnAllIPs: false\n  allowAnonymousLogin: true\n  autopurge:\n    snapRetainCount: 3\n    purgeInterval: 0\n  maxSessionTimeout: 40000\n\n  allowAnonymousLogin: true\n\n  minServerId: 1\n\n  securityContext:\n    enabled: true\n    fsGroup: 1001\n    runAsUser: 1001\n\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n    probeCommandTimeout: 2\n\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 5\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n    probeCommandTimeout: 2\n\n  service:\n    type: ClusterIP\n    # loadBalancerIP: \"\"\n    port: 2181\n    followerPort: 2888\n    electionPort: 3888\n    nodePorts:\n      client: \"\"\n      clientTls: \"\"\n    publishNotReadyAddresses: true\n    tlsClientPort: 3181\n    disableBaseClientPort: false\n    annotations: {}\n    headless:\n      annotations: {}\n"
            ],
            "verify": false,
            "version": "0.1.0",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "helm_release.druid_operator",
            "helm_release.postgres",
            "kind_cluster.one-click"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "druid_operator",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "../druid-operator",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "druid-operator",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.0.7",
                "chart": "druid-operator",
                "name": "druid-operator",
                "namespace": "druid-raw",
                "revision": 1,
                "values": "{}",
                "version": "0.1.1"
              }
            ],
            "name": "druid-operator",
            "namespace": "druid-raw",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "0.1.1",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "helm_release.postgres",
            "kind_cluster.one-click"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "kafka",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "../kafka",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": true,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "kafka",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "3.2.0",
                "chart": "kafka",
                "name": "kafka",
                "namespace": "kafka",
                "revision": 1,
                "values": "{\"advertisedListeners\":[],\"affinity\":{},\"allowEveryoneIfNoAclFound\":true,\"allowPlaintextListener\":true,\"args\":[],\"auth\":{\"clientProtocol\":\"plaintext\",\"externalClientProtocol\":\"\",\"interBrokerProtocol\":\"plaintext\",\"sasl\":{\"interBrokerMechanism\":\"plain\",\"jaas\":{\"clientPasswords\":[],\"clientUsers\":[\"user\"],\"existingSecret\":\"\",\"interBrokerPassword\":\"\",\"interBrokerUser\":\"admin\",\"zookeeperPassword\":\"\",\"zookeeperUser\":\"\"},\"mechanisms\":\"plain,scram-sha-256,scram-sha-512\"},\"tls\":{\"autoGenerated\":false,\"endpointIdentificationAlgorithm\":\"https\",\"existingSecret\":\"\",\"existingSecrets\":[],\"jksKeystoreSAN\":\"\",\"jksTruststore\":\"\",\"jksTruststoreSecret\":\"\",\"password\":\"\",\"pemChainIncluded\":false,\"type\":\"jks\"},\"zookeeper\":{\"tls\":{\"enabled\":false,\"existingSecret\":\"\",\"existingSecretKeystoreKey\":\"zookeeper.keystore.jks\",\"existingSecretTruststoreKey\":\"zookeeper.truststore.jks\",\"passwordsSecret\":\"\",\"passwordsSecretKeystoreKey\":\"keystore-password\",\"passwordsSecretTruststoreKey\":\"truststore-password\",\"type\":\"jks\",\"verifyHostname\":true}}},\"authorizerClassName\":\"\",\"autoCreateTopicsEnable\":true,\"clusterDomain\":\"cluster.local\",\"command\":[\"/scripts/setup.sh\"],\"commonAnnotations\":{},\"commonLabels\":{},\"config\":\"\",\"containerPorts\":{\"client\":9092,\"external\":9094,\"internal\":9093},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"defaultReplicationFactor\":1,\"deleteTopicEnable\":false,\"diagnosticMode\":{\"args\":[\"infinity\"],\"command\":[\"sleep\"],\"enabled\":false},\"existingConfigmap\":\"\",\"existingLog4jConfigMap\":\"\",\"externalAccess\":{\"autoDiscovery\":{\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/kubectl\",\"tag\":\"1.24.2-debian-11-r4\"},\"resources\":{\"limits\":{},\"requests\":{}}},\"enabled\":false,\"service\":{\"annotations\":{},\"domain\":\"\",\"extraPorts\":[],\"loadBalancerAnnotations\":[],\"loadBalancerIPs\":[],\"loadBalancerNames\":[],\"loadBalancerSourceRanges\":[],\"nodePorts\":[],\"ports\":{\"external\":9094},\"type\":\"LoadBalancer\",\"useHostIPs\":false,\"usePodIPs\":false}},\"externalZookeeper\":{\"servers\":[]},\"extraDeploy\":[],\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"storageClass\":\"\"},\"heapOpts\":\"-Xmx1024m -Xms1024m\",\"hostAliases\":[],\"hostIPC\":false,\"hostNetwork\":false,\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/kafka\",\"tag\":\"3.2.0-debian-11-r12\"},\"initContainers\":[],\"interBrokerListenerName\":\"INTERNAL\",\"kubeVersion\":\"\",\"lifecycleHooks\":{},\"listenerSecurityProtocolMap\":\"\",\"listeners\":[],\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"log4j\":\"\",\"logFlushIntervalMessages\":\"_10000\",\"logFlushIntervalMs\":1000,\"logPersistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":false,\"existingClaim\":\"\",\"mountPath\":\"/opt/bitnami/kafka/logs\",\"selector\":{},\"size\":\"2Gi\",\"storageClass\":\"\"},\"logRetentionBytes\":\"_1073741824\",\"logRetentionCheckIntervalMs\":300000,\"logRetentionHours\":168,\"logSegmentBytes\":\"_1073741824\",\"logsDirs\":\"/bitnami/kafka/data\",\"maxMessageBytes\":\"_1000012\",\"metrics\":{\"jmx\":{\"config\":\"jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi\\nlowercaseOutputName: true\\nlowercaseOutputLabelNames: true\\nssl: false\\n{{- if .Values.metrics.jmx.whitelistObjectNames }}\\nwhitelistObjectNames: [\\\"{{ join \\\"\\\\\\\",\\\\\\\"\\\" .Values.metrics.jmx.whitelistObjectNames }}\\\"]\\n{{- end }}\",\"containerPorts\":{\"metrics\":5556},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"enabled\":false,\"existingConfigmap\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/jmx-exporter\",\"tag\":\"0.17.0-debian-11-r12\"},\"resources\":{\"limits\":{},\"requests\":{}},\"service\":{\"annotations\":{\"prometheus.io/path\":\"/\",\"prometheus.io/port\":\"{{ .Values.metrics.jmx.service.ports.metrics }}\",\"prometheus.io/scrape\":\"true\"},\"clusterIP\":\"\",\"ports\":{\"metrics\":5556},\"sessionAffinity\":\"None\"},\"whitelistObjectNames\":[\"kafka.controller:*\",\"kafka.server:*\",\"java.lang:*\",\"kafka.network:*\",\"kafka.log:*\"]},\"kafka\":{\"affinity\":{},\"args\":[],\"certificatesSecret\":\"\",\"command\":[],\"containerPorts\":{\"metrics\":9308},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"enabled\":false,\"extraFlags\":{},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/kafka-exporter\",\"tag\":\"1.4.2-debian-11-r12\"},\"initContainers\":[],\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"service\":{\"annotations\":{\"prometheus.io/path\":\"/metrics\",\"prometheus.io/port\":\"{{ .Values.metrics.kafka.service.ports.metrics }}\",\"prometheus.io/scrape\":\"true\"},\"clusterIP\":\"\",\"ports\":{\"metrics\":9308},\"sessionAffinity\":\"None\"},\"serviceAccount\":{\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"sidecars\":[],\"tlsCaCert\":\"ca-file\",\"tlsCaSecret\":\"\",\"tlsCert\":\"cert-file\",\"tlsKey\":\"key-file\",\"tolerations\":[],\"topologySpreadConstraints\":[]},\"serviceMonitor\":{\"enabled\":false,\"honorLabels\":false,\"interval\":\"\",\"jobLabel\":\"\",\"labels\":{},\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scrapeTimeout\":\"\",\"selector\":{}}},\"minBrokerId\":0,\"nameOverride\":\"\",\"networkPolicy\":{\"allowExternal\":true,\"egressRules\":{\"customRules\":[]},\"enabled\":false,\"explicitNamespacesSelector\":{},\"externalAccess\":{\"from\":[]}},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"numIoThreads\":8,\"numNetworkThreads\":3,\"numPartitions\":1,\"numRecoveryThreadsPerDataDir\":1,\"offsetsTopicReplicationFactor\":1,\"pdb\":{\"create\":false,\"maxUnavailable\":1,\"minAvailable\":\"\"},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"mountPath\":\"/bitnami/kafka\",\"selector\":{},\"size\":\"2Gi\",\"storageClass\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podManagementPolicy\":\"Parallel\",\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"provisioning\":{\"args\":[],\"auth\":{\"tls\":{\"caCert\":\"ca.crt\",\"cert\":\"tls.crt\",\"certificatesSecret\":\"\",\"key\":\"tls.key\",\"keyPassword\":\"\",\"keyPasswordSecretKey\":\"key-password\",\"keystore\":\"keystore.jks\",\"keystorePassword\":\"\",\"keystorePasswordSecretKey\":\"keystore-password\",\"passwordsSecret\":\"\",\"truststore\":\"truststore.jks\",\"truststorePassword\":\"\",\"truststorePasswordSecretKey\":\"truststore-password\",\"type\":\"jks\"}},\"command\":[],\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"enabled\":false,\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraProvisioningCommands\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"initContainers\":[],\"numPartitions\":1,\"parallel\":1,\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"postScript\":\"\",\"preScript\":\"\",\"replicationFactor\":3,\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"sidecars\":[],\"tolerations\":[],\"topics\":[],\"waitForKafka\":true},\"rbac\":{\"create\":false},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":1,\"resources\":{\"limits\":{},\"requests\":{}},\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"extraPorts\":[],\"headless\":{\"annotations\":{},\"labels\":{}},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"client\":\"\",\"external\":\"\"},\"ports\":{\"client\":9092,\"external\":9094,\"internal\":9093},\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{},\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"sidecars\":[],\"socketReceiveBufferBytes\":102400,\"socketRequestMaxBytes\":\"_104857600\",\"socketSendBufferBytes\":102400,\"startupProbe\":{\"enabled\":false,\"failureThreshold\":15,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"superUsers\":\"User:admin\",\"terminationGracePeriodSeconds\":\"\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"transactionStateLogMinIsr\":1,\"transactionStateLogReplicationFactor\":1,\"updateStrategy\":{\"rollingUpdate\":{},\"type\":\"RollingUpdate\"},\"volumePermissions\":{\"containerSecurityContext\":{\"runAsUser\":0},\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/bitnami-shell\",\"tag\":\"11-debian-11-r12\"},\"resources\":{\"limits\":{},\"requests\":{}}},\"zookeeper\":{\"auth\":{\"client\":{\"clientPassword\":\"\",\"clientUser\":\"\",\"enabled\":false,\"serverPasswords\":\"\",\"serverUsers\":\"\"}},\"enabled\":true,\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":true,\"size\":\"2Gi\",\"storageClass\":\"\"},\"replicaCount\":1},\"zookeeperChrootPath\":\"\",\"zookeeperConnectionTimeoutMs\":6000}",
                "version": "18.0.3"
              }
            ],
            "name": "kafka",
            "namespace": "kafka",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "## @section Global parameters\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass\n\n## @param global.imageRegistry Global Docker image registry\n## @param global.imagePullSecrets Global Docker registry secret names as an array\n## @param global.storageClass Global StorageClass for Persistent Volume(s)\n##\nglobal:\n  imageRegistry: \"\"\n  ## E.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  storageClass: \"\"\n\n## @section Common parameters\n\n## @param kubeVersion Override Kubernetes version\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override common.names.fullname\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override common.names.fullname\n##\nfullnameOverride: \"\"\n## @param clusterDomain Default Kubernetes cluster domain\n##\nclusterDomain: cluster.local\n## @param commonLabels Labels to add to all deployed objects\n##\ncommonLabels: {}\n## @param commonAnnotations Annotations to add to all deployed objects\n##\ncommonAnnotations: {}\n## @param extraDeploy Array of extra objects to deploy with the release\n##\nextraDeploy: []\n## Enable diagnostic mode in the statefulset\n##\ndiagnosticMode:\n  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)\n  ##\n  enabled: false\n  ## @param diagnosticMode.command Command to override all containers in the statefulset\n  ##\n  command:\n    - sleep\n  ## @param diagnosticMode.args Args to override all containers in the statefulset\n  ##\n  args:\n    - infinity\n\n## @section Kafka parameters\n\n## Bitnami Kafka image version\n## ref: https://hub.docker.com/r/bitnami/kafka/tags/\n## @param image.registry Kafka image registry\n## @param image.repository Kafka image repository\n## @param image.tag Kafka image tag (immutable tags are recommended)\n## @param image.pullPolicy Kafka image pull policy\n## @param image.pullSecrets Specify docker-registry secret names as an array\n## @param image.debug Specify if debug values should be set\n##\nimage:\n  registry: docker.io\n  repository: bitnami/kafka\n  tag: 3.2.0-debian-11-r12\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## e.g:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ##\n  debug: false\n## @param config Configuration file for Kafka. Auto-generated based on other parameters when not specified\n## Specify content for server.properties\n## NOTE: This will override any KAFKA_CFG_ environment variables (including those set by the chart)\n## The server.properties is auto-generated based on other parameters when this parameter is not specified\n## e.g:\n## config: |-\n##   broker.id=-1\n##   listeners=PLAINTEXT://:9092\n##   advertised.listeners=PLAINTEXT://KAFKA_IP:9092\n##   num.network.threads=3\n##   num.io.threads=8\n##   socket.send.buffer.bytes=102400\n##   socket.receive.buffer.bytes=102400\n##   socket.request.max.bytes=104857600\n##   log.dirs=/bitnami/kafka/data\n##   num.partitions=1\n##   num.recovery.threads.per.data.dir=1\n##   offsets.topic.replication.factor=1\n##   transaction.state.log.replication.factor=1\n##   transaction.state.log.min.isr=1\n##   log.flush.interval.messages=10000\n##   log.flush.interval.ms=1000\n##   log.retention.hours=168\n##   log.retention.bytes=1073741824\n##   log.segment.bytes=1073741824\n##   log.retention.check.interval.ms=300000\n##   zookeeper.connect=ZOOKEEPER_SERVICE_NAME\n##   zookeeper.connection.timeout.ms=6000\n##   group.initial.rebalance.delay.ms=0\n##\nconfig: \"\"\n## @param existingConfigmap ConfigMap with Kafka Configuration\n## NOTE: This will override `config` AND any KAFKA_CFG_ environment variables\n##\nexistingConfigmap: \"\"\n## @param log4j An optional log4j.properties file to overwrite the default of the Kafka brokers\n## An optional log4j.properties file to overwrite the default of the Kafka brokers\n## ref: https://github.com/apache/kafka/blob/trunk/config/log4j.properties\n##\nlog4j: \"\"\n## @param existingLog4jConfigMap The name of an existing ConfigMap containing a log4j.properties file\n## The name of an existing ConfigMap containing a log4j.properties file\n## NOTE: this will override `log4j`\n##\nexistingLog4jConfigMap: \"\"\n## @param heapOpts Kafka Java Heap size\n##\nheapOpts: -Xmx1024m -Xms1024m\n## @param deleteTopicEnable Switch to enable topic deletion or not\n##\ndeleteTopicEnable: false\n## @param autoCreateTopicsEnable Switch to enable auto creation of topics. Enabling auto creation of topics not recommended for production or similar environments\n##\nautoCreateTopicsEnable: true\n## @param logFlushIntervalMessages The number of messages to accept before forcing a flush of data to disk\n##\nlogFlushIntervalMessages: _10000\n## @param logFlushIntervalMs The maximum amount of time a message can sit in a log before we force a flush\n##\nlogFlushIntervalMs: 1000\n## @param logRetentionBytes A size-based retention policy for logs\n##\nlogRetentionBytes: _1073741824\n## @param logRetentionCheckIntervalMs The interval at which log segments are checked to see if they can be deleted\n##\nlogRetentionCheckIntervalMs: 300000\n## @param logRetentionHours The minimum age of a log file to be eligible for deletion due to age\n##\nlogRetentionHours: 168\n## @param logSegmentBytes The maximum size of a log segment file. When this size is reached a new log segment will be created\n##\nlogSegmentBytes: _1073741824\n## @param logsDirs A comma separated list of directories under which to store log files\n##\nlogsDirs: /bitnami/kafka/data\n## @param maxMessageBytes The largest record batch size allowed by Kafka\n##\nmaxMessageBytes: _1000012\n## @param defaultReplicationFactor Default replication factors for automatically created topics\n##\ndefaultReplicationFactor: 1\n## @param offsetsTopicReplicationFactor The replication factor for the offsets topic\n##\noffsetsTopicReplicationFactor: 1\n## @param transactionStateLogReplicationFactor The replication factor for the transaction topic\n##\ntransactionStateLogReplicationFactor: 1\n## @param transactionStateLogMinIsr Overridden min.insync.replicas config for the transaction topic\n##\ntransactionStateLogMinIsr: 1\n## @param numIoThreads The number of threads doing disk I/O\n##\nnumIoThreads: 8\n## @param numNetworkThreads The number of threads handling network requests\n##\nnumNetworkThreads: 3\n## @param numPartitions The default number of log partitions per topic\n##\nnumPartitions: 1\n## @param numRecoveryThreadsPerDataDir The number of threads per data directory to be used for log recovery at startup and flushing at shutdown\n##\nnumRecoveryThreadsPerDataDir: 1\n## @param socketReceiveBufferBytes The receive buffer (SO_RCVBUF) used by the socket server\n##\nsocketReceiveBufferBytes: 102400\n## @param socketRequestMaxBytes The maximum size of a request that the socket server will accept (protection against OOM)\n##\nsocketRequestMaxBytes: _104857600\n## @param socketSendBufferBytes The send buffer (SO_SNDBUF) used by the socket server\n##\nsocketSendBufferBytes: 102400\n## @param zookeeperConnectionTimeoutMs Timeout in ms for connecting to ZooKeeper\n##\nzookeeperConnectionTimeoutMs: 6000\n## @param zookeeperChrootPath Path which puts data under some path in the global ZooKeeper namespace\n## ref: https://kafka.apache.org/documentation/#brokerconfigs_zookeeper.connect\n##\nzookeeperChrootPath: \"\"\n## @param authorizerClassName The Authorizer is configured by setting authorizer.class.name=kafka.security.authorizer.AclAuthorizer in server.properties\n##\nauthorizerClassName: \"\"\n## @param allowEveryoneIfNoAclFound By default, if a resource has no associated ACLs, then no one is allowed to access that resource except super users\n##\nallowEveryoneIfNoAclFound: true\n## @param superUsers You can add super users in server.properties\n##\nsuperUsers: User:admin\n## Authentication parameters\n## https://github.com/bitnami/bitnami-docker-kafka#security\n##\nauth:\n  ## Authentication protocol for client and inter-broker communications\n  ## This table shows the security provided on each protocol:\n  ## | Method    | Authentication                | Encryption via TLS |\n  ## | plaintext | None                          | No                 |\n  ## | tls       | None                          | Yes                |\n  ## | mtls      | Yes (two-way authentication)  | Yes                |\n  ## | sasl      | Yes (via SASL)                | No                 |\n  ## | sasl_tls  | Yes (via SASL)                | Yes                |\n  ## @param auth.clientProtocol Authentication protocol for communications with clients. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ## @param auth.externalClientProtocol Authentication protocol for communications with external clients. Defaults to value of `auth.clientProtocol`. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ## @param auth.interBrokerProtocol Authentication protocol for inter-broker communications. Allowed protocols: `plaintext`, `tls`, `mtls`, `sasl` and `sasl_tls`\n  ##\n  clientProtocol: plaintext\n  # Note: empty by default for backwards compatibility reasons, find more information at\n  # https://github.com/bitnami/charts/pull/8902/\n  externalClientProtocol: \"\"\n  interBrokerProtocol: plaintext\n  ## SASL configuration\n  ##\n  sasl:\n    ## @param auth.sasl.mechanisms SASL mechanisms when either `auth.interBrokerProtocol`, `auth.clientProtocol` or `auth.externalClientProtocol` are `sasl`. Allowed types: `plain`, `scram-sha-256`, `scram-sha-512`\n    ##\n    mechanisms: plain,scram-sha-256,scram-sha-512\n    ## @param auth.sasl.interBrokerMechanism SASL mechanism for inter broker communication.\n    ##\n    interBrokerMechanism: plain\n    ## JAAS configuration for SASL authentication.\n    ##\n    jaas:\n      ## @param auth.sasl.jaas.clientUsers Kafka client user list\n      ##\n      ## clientUsers:\n      ##   - user1\n      ##   - user2\n      ##\n      clientUsers:\n        - user\n      ## @param auth.sasl.jaas.clientPasswords Kafka client passwords. This is mandatory if more than one user is specified in clientUsers\n      ##\n      ## clientPasswords:\n      ##   - password1\n      ##   - password2\"\n      ##\n      clientPasswords: []\n      ## @param auth.sasl.jaas.interBrokerUser Kafka inter broker communication user for SASL authentication\n      ##\n      interBrokerUser: admin\n      ## @param auth.sasl.jaas.interBrokerPassword Kafka inter broker communication password for SASL authentication\n      ##\n      interBrokerPassword: \"\"\n      ## @param auth.sasl.jaas.zookeeperUser Kafka ZooKeeper user for SASL authentication\n      ##\n      zookeeperUser: \"\"\n      ## @param auth.sasl.jaas.zookeeperPassword Kafka ZooKeeper password for SASL authentication\n      ##\n      zookeeperPassword: \"\"\n      ## @param auth.sasl.jaas.existingSecret Name of the existing secret containing credentials for clientUsers, interBrokerUser and zookeeperUser\n      ## Create this secret running the command below where SECRET_NAME is the name of the secret you want to create:\n      ##       kubectl create secret generic SECRET_NAME --from-literal=client-passwords=CLIENT_PASSWORD1,CLIENT_PASSWORD2 --from-literal=inter-broker-password=INTER_BROKER_PASSWORD --from-literal=zookeeper-password=ZOOKEEPER_PASSWORD\n      ##\n      existingSecret: \"\"\n  ## TLS configuration\n  ##\n  tls:\n    ## @param auth.tls.type Format to use for TLS certificates. Allowed types: `jks` and `pem`\n    ##\n    type: jks\n    ## @param auth.tls.pemChainIncluded Flag to denote that the Certificate Authority (CA) certificates are bundled with the endpoint cert.\n    ## Certificates must be in proper order, where the top certificate is the leaf and the bottom certificate is the top-most intermediate CA.\n    ##\n    pemChainIncluded: false\n    ## @param auth.tls.existingSecrets Array existing secrets containing the TLS certificates for the Kafka brokers\n    ## When using 'jks' format for certificates, each secret should contain a truststore and a keystore.\n    ## Create these secrets following the steps below:\n    ## 1) Generate your truststore and keystore files. Helpful script: https://raw.githubusercontent.com/confluentinc/confluent-platform-security-tools/master/kafka-generate-ssl.sh\n    ## 2) Rename your truststore to `kafka.truststore.jks`.\n    ## 3) Rename your keystores to `kafka-X.keystore.jks` where X is the ID of each Kafka broker.\n    ## 4) Run the command below one time per broker to create its associated secret (SECRET_NAME_X is the name of the secret you want to create):\n    ##       kubectl create secret generic SECRET_NAME_0 --from-file=kafka.truststore.jks=./kafka.truststore.jks --from-file=kafka.keystore.jks=./kafka-0.keystore.jks\n    ##       kubectl create secret generic SECRET_NAME_1 --from-file=kafka.truststore.jks=./kafka.truststore.jks --from-file=kafka.keystore.jks=./kafka-1.keystore.jks\n    ##       ...\n    ##\n    ## When using 'pem' format for certificates, each secret should contain a public CA certificate, a public certificate and one private key.\n    ## Create these secrets following the steps below:\n    ## 1) Create a certificate key and signing request per Kafka broker, and sign the signing request with your CA\n    ## 2) Rename your CA file to `kafka.ca.crt`.\n    ## 3) Rename your certificates to `kafka-X.tls.crt` where X is the ID of each Kafka broker.\n    ## 3) Rename your keys to `kafka-X.tls.key` where X is the ID of each Kafka broker.\n    ## 4) Run the command below one time per broker to create its associated secret (SECRET_NAME_X is the name of the secret you want to create):\n    ##       kubectl create secret generic SECRET_NAME_0 --from-file=ca.crt=./kafka.ca.crt --from-file=tls.crt=./kafka-0.tls.crt --from-file=tls.key=./kafka-0.tls.key\n    ##       kubectl create secret generic SECRET_NAME_1 --from-file=ca.crt=./kafka.ca.crt --from-file=tls.crt=./kafka-1.tls.crt --from-file=tls.key=./kafka-1.tls.key\n    ##       ...\n    ##\n    existingSecrets: []\n    ## @param auth.tls.autoGenerated Generate automatically self-signed TLS certificates for Kafka brokers. Currently only supported if `auth.tls.type` is `pem`\n    ## Note: ignored when using 'jks' format or `auth.tls.existingSecrets` is not empty\n    ##\n    autoGenerated: false\n    ## @param auth.tls.password Password to access the JKS files or PEM key when they are password-protected.\n    ## Note: ignored when using 'existingSecret'.\n    ##\n    password: \"\"\n    ## @param auth.tls.existingSecret Name of the secret containing the password to access the JKS files or PEM key when they are password-protected. (`key`: `password`)\n    ##\n    existingSecret: \"\"\n    ## @param auth.tls.jksTruststoreSecret Name of the existing secret containing your truststore if truststore not existing or different from the ones in the `auth.tls.existingSecrets`\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksTruststoreSecret: \"\"\n    ## @param auth.tls.jksKeystoreSAN The secret key from the `auth.tls.existingSecrets` containing the keystore with a SAN certificate\n    ## The SAN certificate in it should be issued with Subject Alternative Names for all headless services:\n    ##  - kafka-0.kafka-headless.kafka.svc.cluster.local\n    ##  - kafka-1.kafka-headless.kafka.svc.cluster.local\n    ##  - kafka-2.kafka-headless.kafka.svc.cluster.local\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksKeystoreSAN: \"\"\n    ## @param auth.tls.jksTruststore The secret key from the `auth.tls.existingSecrets` or `auth.tls.jksTruststoreSecret` containing the truststore\n    ## Note: ignored when using 'pem' format for certificates.\n    ##\n    jksTruststore: \"\"\n    ## @param auth.tls.endpointIdentificationAlgorithm The endpoint identification algorithm to validate server hostname using server certificate\n    ## Disable server host name verification by setting it to an empty string.\n    ## ref: https://docs.confluent.io/current/kafka/authentication_ssl.html#optional-settings\n    ##\n    endpointIdentificationAlgorithm: https\n  ## Zookeeper client configuration for kafka brokers\n  ##\n  zookeeper:\n    ## TLS configuration\n    ##\n    tls:\n      ## @param auth.zookeeper.tls.enabled Enable TLS for Zookeeper client connections.\n      ##\n      enabled: false\n      ## @param auth.zookeeper.tls.type Format to use for TLS certificates. Allowed types: `jks` and `pem`.\n      ##\n      type: jks\n      ## @param auth.zookeeper.tls.verifyHostname Hostname validation.\n      ##\n      verifyHostname: true\n      ## @param auth.zookeeper.tls.existingSecret Name of the existing secret containing the TLS certificates for ZooKeeper client communications.\n      ##\n      existingSecret: \"\"\n      ## @param auth.zookeeper.tls.existingSecretKeystoreKey The secret key from the  auth.zookeeper.tls.existingSecret containing the Keystore.\n      ##\n      existingSecretKeystoreKey: zookeeper.keystore.jks\n      ## @param auth.zookeeper.tls.existingSecretTruststoreKey The secret key from the auth.zookeeper.tls.existingSecret containing the Truststore.\n      ##\n      existingSecretTruststoreKey: zookeeper.truststore.jks\n      ## @param auth.zookeeper.tls.passwordsSecret Existing secret containing Keystore and Truststore passwords.\n      ##\n      passwordsSecret: \"\"\n      ## @param auth.zookeeper.tls.passwordsSecretKeystoreKey The secret key from the auth.zookeeper.tls.passwordsSecret containing the password for the Keystore.\n      ##\n      passwordsSecretKeystoreKey: keystore-password\n      ## @param auth.zookeeper.tls.passwordsSecretTruststoreKey The secret key from the auth.zookeeper.tls.passwordsSecret containing the password for the Truststore.\n      ##\n      passwordsSecretTruststoreKey: truststore-password\n## @param listeners The address(es) the socket server listens on. Auto-calculated it's set to an empty array\n## When it's set to an empty array, the listeners will be configured\n## based on the authentication protocols (auth.clientProtocol, auth.externalClientProtocol and auth.interBrokerProtocol parameters)\n##\nlisteners: []\n## @param advertisedListeners The address(es) (hostname:port) the broker will advertise to producers and consumers. Auto-calculated it's set to an empty array\n## When it's set to an empty array, the advertised listeners will be configured\n## based on the authentication protocols (auth.clientProtocol, auth.externalClientProtocol and auth.interBrokerProtocol parameters)\n##\nadvertisedListeners: []\n## @param listenerSecurityProtocolMap The protocol-\u003elistener mapping. Auto-calculated it's set to nil\n## When it's nil, the listeners will be configured based on the authentication protocols (auth.clientProtocol, auth.externalClientProtocol and auth.interBrokerProtocol parameters)\n##\nlistenerSecurityProtocolMap: \"\"\n## @param allowPlaintextListener Allow to use the PLAINTEXT listener\n##\nallowPlaintextListener: true\n## @param interBrokerListenerName The listener that the brokers should communicate on\n##\ninterBrokerListenerName: INTERNAL\n## @param command Override Kafka container command\n##\ncommand:\n  - /scripts/setup.sh\n## @param args Override Kafka container arguments\n##\nargs: []\n## @param extraEnvVars Extra environment variables to add to Kafka pods\n## ref: https://github.com/bitnami/bitnami-docker-kafka#configuration\n## e.g:\n## extraEnvVars:\n##   - name: KAFKA_CFG_BACKGROUND_THREADS\n##     value: \"10\"\n##\nextraEnvVars: []\n## @param extraEnvVarsCM ConfigMap with extra environment variables\n##\nextraEnvVarsCM: \"\"\n## @param extraEnvVarsSecret Secret with extra environment variables\n##\nextraEnvVarsSecret: \"\"\n\n## @section Statefulset parameters\n\n## @param replicaCount Number of Kafka nodes\n##\nreplicaCount: 1\n## @param minBrokerId Minimal broker.id value, nodes increment their `broker.id` respectively\n## Brokers increment their ID starting at this minimal value.\n## E.g., with `minBrokerId=100` and 3 nodes, IDs will be 100, 101, 102 for brokers 0, 1, and 2, respectively.\n##\nminBrokerId: 0\n## @param containerPorts.client Kafka client container port\n## @param containerPorts.internal Kafka inter-broker container port\n## @param containerPorts.external Kafka external container port\n##\ncontainerPorts:\n  client: 9092\n  internal: 9093\n  external: 9094\n## Configure extra options for Kafka containers' liveness, readiness and startup probes\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\n## @param livenessProbe.enabled Enable livenessProbe on Kafka containers\n## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n## @param livenessProbe.periodSeconds Period seconds for livenessProbe\n## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n## @param livenessProbe.failureThreshold Failure threshold for livenessProbe\n## @param livenessProbe.successThreshold Success threshold for livenessProbe\n##\nlivenessProbe:\n  enabled: true\n  initialDelaySeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n  periodSeconds: 10\n  successThreshold: 1\n## @param readinessProbe.enabled Enable readinessProbe on Kafka containers\n## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n## @param readinessProbe.periodSeconds Period seconds for readinessProbe\n## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n## @param readinessProbe.failureThreshold Failure threshold for readinessProbe\n## @param readinessProbe.successThreshold Success threshold for readinessProbe\n##\nreadinessProbe:\n  enabled: true\n  initialDelaySeconds: 5\n  failureThreshold: 6\n  timeoutSeconds: 5\n  periodSeconds: 10\n  successThreshold: 1\n## @param startupProbe.enabled Enable startupProbe on Kafka containers\n## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n## @param startupProbe.periodSeconds Period seconds for startupProbe\n## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe\n## @param startupProbe.failureThreshold Failure threshold for startupProbe\n## @param startupProbe.successThreshold Success threshold for startupProbe\n##\nstartupProbe:\n  enabled: false\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 1\n  failureThreshold: 15\n  successThreshold: 1\n## @param customLivenessProbe Custom livenessProbe that overrides the default one\n##\ncustomLivenessProbe: {}\n## @param customReadinessProbe Custom readinessProbe that overrides the default one\n##\ncustomReadinessProbe: {}\n## @param customStartupProbe Custom startupProbe that overrides the default one\n##\ncustomStartupProbe: {}\n## @param lifecycleHooks lifecycleHooks for the Kafka container to automate configuration before or after startup\n##\nlifecycleHooks: {}\n## Kafka resource requests and limits\n## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n## @param resources.limits The resources limits for the container\n## @param resources.requests The requested resources for the container\n##\nresources:\n  limits: {}\n  requests: {}\n## Kafka pods' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n## @param podSecurityContext.enabled Enable security context for the pods\n## @param podSecurityContext.fsGroup Set Kafka pod's Security Context fsGroup\n##\npodSecurityContext:\n  enabled: true\n  fsGroup: 1001\n## Kafka containers' Security Context\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n## @param containerSecurityContext.enabled Enable Kafka containers' Security Context\n## @param containerSecurityContext.runAsUser Set Kafka containers' Security Context runAsUser\n## @param containerSecurityContext.runAsNonRoot Set Kafka containers' Security Context runAsNonRoot\n## e.g:\n##   containerSecurityContext:\n##     enabled: true\n##     capabilities:\n##       drop: [\"NET_RAW\"]\n##     readOnlyRootFilesystem: true\n##\ncontainerSecurityContext:\n  enabled: true\n  runAsUser: 1001\n  runAsNonRoot: true\n## @param hostAliases Kafka pods host aliases\n## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n##\nhostAliases: []\n## @param hostNetwork Specify if host network should be enabled for Kafka pods\n##\nhostNetwork: false\n## @param hostIPC Specify if host IPC should be enabled for Kafka pods\n##\nhostIPC: false\n## @param podLabels Extra labels for Kafka pods\n## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n##\npodLabels: {}\n## @param podAnnotations Extra annotations for Kafka pods\n## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n##\npodAnnotations: {}\n## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAffinityPreset: \"\"\n## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n##\npodAntiAffinityPreset: soft\n## Node affinity preset\n## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n##\nnodeAffinityPreset:\n  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`\n  ##\n  type: \"\"\n  ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.\n  ## E.g.\n  ## key: \"kubernetes.io/e2e-az-name\"\n  ##\n  key: \"\"\n  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.\n  ## E.g.\n  ## values:\n  ##   - e2e-az1\n  ##   - e2e-az2\n  ##\n  values: []\n## @param affinity Affinity for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set\n##\naffinity: {}\n## @param nodeSelector Node labels for pod assignment\n## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n##\nnodeSelector: {}\n## @param tolerations Tolerations for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n##\ntolerations: []\n## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template\n## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n##\ntopologySpreadConstraints: []\n## @param terminationGracePeriodSeconds Seconds the pod needs to gracefully terminate\n## ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-execution\n##\nterminationGracePeriodSeconds: \"\"\n## @param podManagementPolicy StatefulSet controller supports relax its ordering guarantees while preserving its uniqueness and identity guarantees. There are two valid pod management policies: OrderedReady and Parallel\n## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy\n##\npodManagementPolicy: Parallel\n## @param priorityClassName Name of the existing priority class to be used by kafka pods\n## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n##\npriorityClassName: \"\"\n## @param schedulerName Name of the k8s scheduler (other than default)\n## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n##\nschedulerName: \"\"\n## @param updateStrategy.type Kafka statefulset strategy type\n## @param updateStrategy.rollingUpdate Kafka statefulset rolling update configuration parameters\n## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n##\nupdateStrategy:\n  type: RollingUpdate\n  rollingUpdate: {}\n## @param extraVolumes Optionally specify extra list of additional volumes for the Kafka pod(s)\n## e.g:\n## extraVolumes:\n##   - name: kafka-jaas\n##     secret:\n##       secretName: kafka-jaas\n##\nextraVolumes: []\n## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Kafka container(s)\n## extraVolumeMounts:\n##   - name: kafka-jaas\n##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n##     subPath: kafka_jaas.conf\n##\nextraVolumeMounts: []\n## @param sidecars Add additional sidecar containers to the Kafka pod(s)\n## e.g:\n## sidecars:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\nsidecars: []\n## @param initContainers Add additional Add init containers to the Kafka pod(s)\n## e.g:\n## initContainers:\n##   - name: your-image-name\n##     image: your-image\n##     imagePullPolicy: Always\n##     ports:\n##       - name: portname\n##         containerPort: 1234\n##\ninitContainers: []\n## Kafka Pod Disruption Budget\n## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n## @param pdb.create Deploy a pdb object for the Kafka pod\n## @param pdb.minAvailable Maximum number/percentage of unavailable Kafka replicas\n## @param pdb.maxUnavailable Maximum number/percentage of unavailable Kafka replicas\n##\npdb:\n  create: false\n  minAvailable: \"\"\n  maxUnavailable: 1\n\n## @section Traffic Exposure parameters\n\n## Service parameters\n##\nservice:\n  ## @param service.type Kubernetes Service type\n  ##\n  type: \"ClusterIP\" \n  ## @param service.ports.client Kafka svc port for client connections\n  ## @param service.ports.internal Kafka svc port for inter-broker connections\n  ## @param service.ports.external Kafka svc port for external connections\n  ##\n  ports:\n    client: 9092\n    internal: 9093\n    external: 9094\n  ## @param service.nodePorts.client Node port for the Kafka client connections\n  ## @param service.nodePorts.external Node port for the Kafka external connections\n  ## NOTE: choose port between \u003c30000-32767\u003e\n  ##\n  nodePorts:\n    client: \"\"\n    external: \"\"\n  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin\n  ## Values: ClientIP or None\n  ## ref: https://kubernetes.io/docs/user-guide/services/\n  ##\n  sessionAffinity: None\n  ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity\n  ## sessionAffinityConfig:\n  ##   clientIP:\n  ##     timeoutSeconds: 300\n  ##\n  sessionAffinityConfig: {}\n  ## @param service.clusterIP Kafka service Cluster IP\n  ## e.g.:\n  ## clusterIP: None\n  ##\n  clusterIP: \"\"\n  ## @param service.loadBalancerIP Kafka service Load Balancer IP\n  ## ref: https://kubernetes.io/docs/user-guide/services/#type-loadbalancer\n  ##\n  loadBalancerIP: \"\"\n  ## @param service.loadBalancerSourceRanges Kafka service Load Balancer sources\n  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n  ## e.g:\n  ## loadBalancerSourceRanges:\n  ##   - 10.10.10.0/24\n  ##\n  loadBalancerSourceRanges: []\n  ## @param service.externalTrafficPolicy Kafka service external traffic policy\n  ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n  ##\n  externalTrafficPolicy: Cluster\n  ## @param service.annotations Additional custom annotations for Kafka service\n  ##\n  annotations: {}\n  ## Headless service properties\n  ##\n  headless:\n    ## @param service.headless.annotations Annotations for the headless service.\n    ##\n    annotations: {}\n    ## @param service.headless.labels Labels for the headless service.\n    ##\n    labels: {}\n  ## @param service.extraPorts Extra ports to expose in the Kafka service (normally used with the `sidecar` value)\n  ##\n  extraPorts: []\n## External Access to Kafka brokers configuration\n##\nexternalAccess:\n  ## @param externalAccess.enabled Enable Kubernetes external cluster access to Kafka brokers\n  ##\n  enabled: false\n  ## External IPs auto-discovery configuration\n  ## An init container is used to auto-detect LB IPs or node ports by querying the K8s API\n  ## Note: RBAC might be required\n  ##\n  autoDiscovery:\n    ## @param externalAccess.autoDiscovery.enabled Enable using an init container to auto-detect external IPs/ports by querying the K8s API\n    ##\n    enabled: false\n    ## Bitnami Kubectl image\n    ## ref: https://hub.docker.com/r/bitnami/kubectl/tags/\n    ## @param externalAccess.autoDiscovery.image.registry Init container auto-discovery image registry\n    ## @param externalAccess.autoDiscovery.image.repository Init container auto-discovery image repository\n    ## @param externalAccess.autoDiscovery.image.tag Init container auto-discovery image tag (immutable tags are recommended)\n    ## @param externalAccess.autoDiscovery.image.pullPolicy Init container auto-discovery image pull policy\n    ## @param externalAccess.autoDiscovery.image.pullSecrets Init container auto-discovery image pull secrets\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/kubectl\n      tag: 1.24.2-debian-11-r4\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## e.g:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n    ## Init Container resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## @param externalAccess.autoDiscovery.resources.limits The resources limits for the auto-discovery init container\n    ## @param externalAccess.autoDiscovery.resources.requests The requested resources for the auto-discovery init container\n    ##\n    resources:\n      limits: {}\n      requests: {}\n  ## Parameters to configure K8s service(s) used to externally access Kafka brokers\n  ## Note: A new service per broker will be created\n  ##\n  service:\n    ## @param externalAccess.service.type Kubernetes Service type for external access. It can be NodePort or LoadBalancer\n    ##\n    type: LoadBalancer\n    ## @param externalAccess.service.ports.external Kafka port used for external access when service type is LoadBalancer\n    ##\n    ports:\n      external: 9094\n    ## @param externalAccess.service.loadBalancerIPs Array of load balancer IPs for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## loadBalancerIPs:\n    ##   - X.X.X.X\n    ##   - Y.Y.Y.Y\n    ##\n    loadBalancerIPs: []\n    ## @param externalAccess.service.loadBalancerNames Array of load balancer Names for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## loadBalancerNames:\n    ##   - broker1.external.example.com\n    ##   - broker2.external.example.com\n    ##\n    loadBalancerNames: []\n    ## @param externalAccess.service.loadBalancerAnnotations Array of load balancer annotations for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## loadBalancerAnnotations:\n    ##   - external-dns.alpha.kubernetes.io/hostname: broker1.external.example.com.\n    ##   - external-dns.alpha.kubernetes.io/hostname: broker2.external.example.com.\n    ##\n    loadBalancerAnnotations: []\n    ## @param externalAccess.service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer\n    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ## e.g:\n    ## loadBalancerSourceRanges:\n    ## - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param externalAccess.service.nodePorts Array of node ports used for each Kafka broker. Length must be the same as replicaCount\n    ## e.g:\n    ## nodePorts:\n    ##   - 30001\n    ##   - 30002\n    ##\n    nodePorts: []\n    ## @param externalAccess.service.useHostIPs Use service host IPs to configure Kafka external listener when service type is NodePort\n    ##\n    useHostIPs: false\n    ## @param externalAccess.service.usePodIPs using the MY_POD_IP address for external access.\n    ##\n    usePodIPs: false\n    ## @param externalAccess.service.domain Domain or external ip used to configure Kafka external listener when service type is NodePort\n    ## If not specified, the container will try to get the kubernetes node external IP\n    ##\n    domain: \"\"\n    ## @param externalAccess.service.annotations Service annotations for external access\n    ##\n    annotations: {}\n    ## @param externalAccess.service.extraPorts Extra ports to expose in the Kafka external service\n    ##\n    extraPorts: []\n## Network policies\n## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/\n##\nnetworkPolicy:\n  ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created\n  ##\n  enabled: false\n  ## @param networkPolicy.allowExternal Don't require client label for connections\n  ## When set to false, only pods with the correct client label will have network access to the port Kafka is\n  ## listening on. When true, zookeeper accept connections from any source (with the correct destination port).\n  ##\n  allowExternal: true\n  ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed\n  ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace\n  ## and that match other criteria, the ones that have the good label, can reach the kafka.\n  ## But sometimes, we want the kafka to be accessible to clients from other namespaces, in this case, we can use this\n  ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.\n  ##\n  ## e.g:\n  ## explicitNamespacesSelector:\n  ##   matchLabels:\n  ##     role: frontend\n  ##   matchExpressions:\n  ##    - {key: role, operator: In, values: [frontend]}\n  ##\n  explicitNamespacesSelector: {}\n  ## @param networkPolicy.externalAccess.from customize the from section for External Access on tcp-external port\n  ## e.g:\n  ## - ipBlock:\n  ##    cidr: 172.9.0.0/16\n  ##    except:\n  ##    - 172.9.1.0/24\n  ##\n  externalAccess:\n    from: []\n  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule\n  ##\n  egressRules:\n    ## Additional custom egress rules\n    ## e.g:\n    ## customRules:\n    ##   - to:\n    ##       - namespaceSelector:\n    ##           matchLabels:\n    ##             label: example\n    customRules: []\n\n## @section Persistence parameters\n\n## Enable persistence using Persistent Volume Claims\n## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n##\npersistence:\n  ## @param persistence.enabled Enable Kafka data persistence using PVC, note that ZooKeeper persistence is unaffected\n  ##\n  enabled: true\n  ## @param persistence.existingClaim A manually managed Persistent Volume and Claim\n  ## If defined, PVC must be created manually before volume will be bound\n  ## The value is evaluated as a template\n  ##\n  existingClaim: \"\"\n  ## @param persistence.storageClass PVC Storage Class for Kafka data volume\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ## set, choosing the default provisioner.\n  ##\n  storageClass: \"\"\n  ## @param persistence.accessModes Persistent Volume Access Modes\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param persistence.size PVC Storage Request for Kafka data volume\n  ##\n  size: 2Gi\n  ## @param persistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param persistence.selector Selector to match an existing Persistent Volume for Kafka data PVC. If set, the PVC can't have a PV dynamically provisioned for it\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  ##\n  selector: {}\n  ## @param persistence.mountPath Mount path of the Kafka data volume\n  ##\n  mountPath: /bitnami/kafka\n## Log Persistence parameters\n##\nlogPersistence:\n  ## @param logPersistence.enabled Enable Kafka logs persistence using PVC, note that ZooKeeper persistence is unaffected\n  ##\n  enabled: false\n  ## @param logPersistence.existingClaim A manually managed Persistent Volume and Claim\n  ## If defined, PVC must be created manually before volume will be bound\n  ## The value is evaluated as a template\n  ##\n  existingClaim: \"\"\n  ## @param logPersistence.storageClass PVC Storage Class for Kafka logs volume\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ## set, choosing the default provisioner.\n  ##\n  storageClass: \"\"\n  ## @param logPersistence.accessModes Persistent Volume Access Modes\n  ##\n  accessModes:\n    - ReadWriteOnce\n  ## @param logPersistence.size PVC Storage Request for Kafka logs volume\n  ##\n  size: 2Gi\n  ## @param logPersistence.annotations Annotations for the PVC\n  ##\n  annotations: {}\n  ## @param logPersistence.selector Selector to match an existing Persistent Volume for Kafka log data PVC. If set, the PVC can't have a PV dynamically provisioned for it\n  ## selector:\n  ##   matchLabels:\n  ##     app: my-app\n  ##\n  selector: {}\n  ## @param logPersistence.mountPath Mount path of the Kafka logs volume\n  ##\n  mountPath: /opt/bitnami/kafka/logs\n\n## @section Volume Permissions parameters\n##\n\n## Init containers parameters:\n## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node\n##\nvolumePermissions:\n  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume\n  ##\n  enabled: false\n  ## @param volumePermissions.image.registry Init container volume-permissions image registry\n  ## @param volumePermissions.image.repository Init container volume-permissions image repository\n  ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)\n  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy\n  ## @param volumePermissions.image.pullSecrets Init container volume-permissions image pull secrets\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: 11-debian-11-r12\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Init container resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param volumePermissions.resources.limits Init container volume-permissions resource limits\n  ## @param volumePermissions.resources.requests Init container volume-permissions resource requests\n  ##\n  resources:\n    limits: {}\n    requests: {}\n  ## Init container' Security Context\n  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser\n  ## and not the below volumePermissions.containerSecurityContext.runAsUser\n  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container\n  ##\n  containerSecurityContext:\n    runAsUser: 0\n\n## @section Other Parameters\n\n## ServiceAccount for Kafka\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable creation of ServiceAccount for Kafka pods\n  ##\n  create: true\n  ## @param serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated\n  ## If not set and create is true, a name is generated using the kafka.serviceAccountName template\n  ##\n  name: \"\"\n  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n  ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n  ##\n  automountServiceAccountToken: true\n  ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount\n  ##\n  annotations: {}\n## Role Based Access Control\n## ref: https://kubernetes.io/docs/admin/authorization/rbac/\n##\nrbac:\n  ## @param rbac.create Whether to create \u0026 use RBAC resources or not\n  ## binding Kafka ServiceAccount to a role\n  ## that allows Kafka pods querying the K8s API\n  ##\n  create: false\n\n## @section Metrics parameters\n\n## Prometheus Exporters / Metrics\n##\nmetrics:\n  ## Prometheus Kafka exporter: exposes complimentary metrics to JMX exporter\n  ##\n  kafka:\n    ## @param metrics.kafka.enabled Whether or not to create a standalone Kafka exporter to expose Kafka metrics\n    ##\n    enabled: false\n    ## Bitnami Kafka exporter image\n    ## ref: https://hub.docker.com/r/bitnami/kafka-exporter/tags/\n    ## @param metrics.kafka.image.registry Kafka exporter image registry\n    ## @param metrics.kafka.image.repository Kafka exporter image repository\n    ## @param metrics.kafka.image.tag Kafka exporter image tag (immutable tags are recommended)\n    ## @param metrics.kafka.image.pullPolicy Kafka exporter image pull policy\n    ## @param metrics.kafka.image.pullSecrets Specify docker-registry secret names as an array\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/kafka-exporter\n      tag: 1.4.2-debian-11-r12\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## e.g:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n\n    ## @param metrics.kafka.certificatesSecret Name of the existing secret containing the optional certificate and key files\n    ## for Kafka exporter client authentication\n    ##\n    certificatesSecret: \"\"\n    ## @param metrics.kafka.tlsCert The secret key from the certificatesSecret if 'client-cert' key different from the default (cert-file)\n    ##\n    tlsCert: cert-file\n    ## @param metrics.kafka.tlsKey The secret key from the certificatesSecret if 'client-key' key different from the default (key-file)\n    ##\n    tlsKey: key-file\n    ## @param metrics.kafka.tlsCaSecret Name of the existing secret containing the optional ca certificate for Kafka exporter client authentication\n    ##\n    tlsCaSecret: \"\"\n    ## @param metrics.kafka.tlsCaCert The secret key from the certificatesSecret or tlsCaSecret if 'ca-cert' key different from the default (ca-file)\n    ##\n    tlsCaCert: ca-file\n    ## @param metrics.kafka.extraFlags Extra flags to be passed to Kafka exporter\n    ## e.g:\n    ## extraFlags:\n    ##   tls.insecure-skip-tls-verify: \"\"\n    ##   web.telemetry-path: \"/metrics\"\n    ##\n    extraFlags: {}\n    ## @param metrics.kafka.command Override Kafka exporter container command\n    ##\n    command: []\n    ## @param metrics.kafka.args Override Kafka exporter container arguments\n    ##\n    args: []\n    ## @param metrics.kafka.containerPorts.metrics Kafka exporter metrics container port\n    ##\n    containerPorts:\n      metrics: 9308\n    ## Kafka exporter resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## @param metrics.kafka.resources.limits The resources limits for the container\n    ## @param metrics.kafka.resources.requests The requested resources for the container\n    ##\n    resources:\n      limits: {}\n      requests: {}\n    ## Kafka exporter pods' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n    ## @param metrics.kafka.podSecurityContext.enabled Enable security context for the pods\n    ## @param metrics.kafka.podSecurityContext.fsGroup Set Kafka exporter pod's Security Context fsGroup\n    ##\n    podSecurityContext:\n      enabled: true\n      fsGroup: 1001\n    ## Kafka exporter containers' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n    ## @param metrics.kafka.containerSecurityContext.enabled Enable Kafka exporter containers' Security Context\n    ## @param metrics.kafka.containerSecurityContext.runAsUser Set Kafka exporter containers' Security Context runAsUser\n    ## @param metrics.kafka.containerSecurityContext.runAsNonRoot Set Kafka exporter containers' Security Context runAsNonRoot\n    ## e.g:\n    ##   containerSecurityContext:\n    ##     enabled: true\n    ##     capabilities:\n    ##       drop: [\"NET_RAW\"]\n    ##     readOnlyRootFilesystem: true\n    ##\n    containerSecurityContext:\n      enabled: true\n      runAsUser: 1001\n      runAsNonRoot: true\n    ## @param metrics.kafka.hostAliases Kafka exporter pods host aliases\n    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n    ##\n    hostAliases: []\n    ## @param metrics.kafka.podLabels Extra labels for Kafka exporter pods\n    ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n    ##\n    podLabels: {}\n    ## @param metrics.kafka.podAnnotations Extra annotations for Kafka exporter pods\n    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\n    ##\n    podAnnotations: {}\n    ## @param metrics.kafka.podAffinityPreset Pod affinity preset. Ignored if `metrics.kafka.affinity` is set. Allowed values: `soft` or `hard`\n    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n    ##\n    podAffinityPreset: \"\"\n    ## @param metrics.kafka.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `metrics.kafka.affinity` is set. Allowed values: `soft` or `hard`\n    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n    ##\n    podAntiAffinityPreset: soft\n    ## Node metrics.kafka.affinity preset\n    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n    ##\n    nodeAffinityPreset:\n      ## @param metrics.kafka.nodeAffinityPreset.type Node affinity preset type. Ignored if `metrics.kafka.affinity` is set. Allowed values: `soft` or `hard`\n      ##\n      type: \"\"\n      ## @param metrics.kafka.nodeAffinityPreset.key Node label key to match Ignored if `metrics.kafka.affinity` is set.\n      ## E.g.\n      ## key: \"kubernetes.io/e2e-az-name\"\n      ##\n      key: \"\"\n      ## @param metrics.kafka.nodeAffinityPreset.values Node label values to match. Ignored if `metrics.kafka.affinity` is set.\n      ## E.g.\n      ## values:\n      ##   - e2e-az1\n      ##   - e2e-az2\n      ##\n      values: []\n    ## @param metrics.kafka.affinity Affinity for pod assignment\n    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n    ## Note: metrics.kafka.podAffinityPreset, metrics.kafka.podAntiAffinityPreset, and metrics.kafka.nodeAffinityPreset will be ignored when it's set\n    ##\n    affinity: {}\n    ## @param metrics.kafka.nodeSelector Node labels for pod assignment\n    ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n    ## @param metrics.kafka.tolerations Tolerations for pod assignment\n    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    ## @param metrics.kafka.schedulerName Name of the k8s scheduler (other than default) for Kafka exporter\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n    ##\n    schedulerName: \"\"\n    ## @param metrics.kafka.priorityClassName Kafka exporter pods' priorityClassName\n    ##\n    priorityClassName: \"\"\n    ## @param metrics.kafka.topologySpreadConstraints Topology Spread Constraints for pod assignment\n    ## https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ## The value is evaluated as a template\n    ##\n    topologySpreadConstraints: []\n    ## @param metrics.kafka.extraVolumes Optionally specify extra list of additional volumes for the Kafka exporter pod(s)\n    ## e.g:\n    ## extraVolumes:\n    ##   - name: kafka-jaas\n    ##     secret:\n    ##       secretName: kafka-jaas\n    ##\n    extraVolumes: []\n    ## @param metrics.kafka.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Kafka exporter container(s)\n    ## extraVolumeMounts:\n    ##   - name: kafka-jaas\n    ##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n    ##     subPath: kafka_jaas.conf\n    ##\n    extraVolumeMounts: []\n    ## @param metrics.kafka.sidecars Add additional sidecar containers to the Kafka exporter pod(s)\n    ## e.g:\n    ## sidecars:\n    ##   - name: your-image-name\n    ##     image: your-image\n    ##     imagePullPolicy: Always\n    ##     ports:\n    ##       - name: portname\n    ##         containerPort: 1234\n    ##\n    sidecars: []\n    ## @param metrics.kafka.initContainers Add init containers to the Kafka exporter pods\n    ## e.g:\n    ## initContainers:\n    ##   - name: your-image-name\n    ##     image: your-image\n    ##     imagePullPolicy: Always\n    ##     ports:\n    ##       - name: portname\n    ##         containerPort: 1234\n    ##\n    initContainers: []\n    ## Kafka exporter service configuration\n    ##\n    service:\n      ## @param metrics.kafka.service.ports.metrics Kafka exporter metrics service port\n      ##\n      ports:\n        metrics: 9308\n      ## @param metrics.kafka.service.clusterIP Static clusterIP or None for headless services\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      ##\n      clusterIP: \"\"\n      ## @param metrics.kafka.service.sessionAffinity Control where client requests go, to the same pod or round-robin\n      ## Values: ClientIP or None\n      ## ref: https://kubernetes.io/docs/user-guide/services/\n      ##\n      sessionAffinity: None\n      ## @param metrics.kafka.service.annotations [object] Annotations for the Kafka exporter service\n      ##\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.metrics.kafka.service.ports.metrics }}\"\n        prometheus.io/path: \"/metrics\"\n    ## Kafka exporter pods ServiceAccount\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n    ##\n    serviceAccount:\n      ## @param metrics.kafka.serviceAccount.create Enable creation of ServiceAccount for Kafka exporter pods\n      ##\n      create: true\n      ## @param metrics.kafka.serviceAccount.name The name of the service account to use. If not set and `create` is `true`, a name is generated\n      ## If not set and create is true, a name is generated using the kafka.metrics.kafka.serviceAccountName template\n      ##\n      name: \"\"\n      ## @param metrics.kafka.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n      ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n      ##\n      automountServiceAccountToken: true\n  ## Prometheus JMX exporter: exposes the majority of Kafkas metrics\n  ##\n  jmx:\n    ## @param metrics.jmx.enabled Whether or not to expose JMX metrics to Prometheus\n    ##\n    enabled: false\n    ## Bitnami JMX exporter image\n    ## ref: https://hub.docker.com/r/bitnami/jmx-exporter/tags/\n    ## @param metrics.jmx.image.registry JMX exporter image registry\n    ## @param metrics.jmx.image.repository JMX exporter image repository\n    ## @param metrics.jmx.image.tag JMX exporter image tag (immutable tags are recommended)\n    ## @param metrics.jmx.image.pullPolicy JMX exporter image pull policy\n    ## @param metrics.jmx.image.pullSecrets Specify docker-registry secret names as an array\n    ##\n    image:\n      registry: docker.io\n      repository: bitnami/jmx-exporter\n      tag: 0.17.0-debian-11-r12\n      ## Specify a imagePullPolicy\n      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n      ##\n      pullPolicy: IfNotPresent\n      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n      ## e.g:\n      ## pullSecrets:\n      ##   - myRegistryKeySecretName\n      ##\n      pullSecrets: []\n    ## Prometheus JMX exporter containers' Security Context\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n    ## @param metrics.jmx.containerSecurityContext.enabled Enable Prometheus JMX exporter containers' Security Context\n    ## @param metrics.jmx.containerSecurityContext.runAsUser Set Prometheus JMX exporter containers' Security Context runAsUser\n    ## @param metrics.jmx.containerSecurityContext.runAsNonRoot Set Prometheus JMX exporter containers' Security Context runAsNonRoot\n    ## e.g:\n    ##   containerSecurityContext:\n    ##     enabled: true\n    ##     capabilities:\n    ##       drop: [\"NET_RAW\"]\n    ##     readOnlyRootFilesystem: true\n    ##\n    containerSecurityContext:\n      enabled: true\n      runAsUser: 1001\n      runAsNonRoot: true\n    ## @param metrics.jmx.containerPorts.metrics Prometheus JMX exporter metrics container port\n    ##\n    containerPorts:\n      metrics: 5556\n    ## Prometheus JMX exporter resource requests and limits\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ## @param metrics.jmx.resources.limits The resources limits for the JMX exporter container\n    ## @param metrics.jmx.resources.requests The requested resources for the JMX exporter container\n    ##\n    resources:\n      limits: {}\n      requests: {}\n    ## Prometheus JMX exporter service configuration\n    ##\n    service:\n      ## @param metrics.jmx.service.ports.metrics Prometheus JMX exporter metrics service port\n      ##\n      ports:\n        metrics: 5556\n      ## @param metrics.jmx.service.clusterIP Static clusterIP or None for headless services\n      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n      ##\n      clusterIP: \"\"\n      ## @param metrics.jmx.service.sessionAffinity Control where client requests go, to the same pod or round-robin\n      ## Values: ClientIP or None\n      ## ref: https://kubernetes.io/docs/user-guide/services/\n      ##\n      sessionAffinity: None\n      ## @param metrics.jmx.service.annotations [object] Annotations for the Prometheus JMX exporter service\n      ##\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.metrics.jmx.service.ports.metrics }}\"\n        prometheus.io/path: \"/\"\n    ## @param metrics.jmx.whitelistObjectNames Allows setting which JMX objects you want to expose to via JMX stats to JMX exporter\n    ## Only whitelisted values will be exposed via JMX exporter. They must also be exposed via Rules. To expose all metrics\n    ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`\n    ## (2) commented out above `overrideConfig`.\n    ##\n    whitelistObjectNames:\n      - kafka.controller:*\n      - kafka.server:*\n      - java.lang:*\n      - kafka.network:*\n      - kafka.log:*\n    ## @param metrics.jmx.config [string] Configuration file for JMX exporter\n    ## Specify content for jmx-kafka-prometheus.yml. Evaluated as a template\n    ##\n    ## Credits to the incubator/kafka chart for the JMX configuration.\n    ## https://github.com/helm/charts/tree/master/incubator/kafka\n    ##\n    config: |-\n      jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi\n      lowercaseOutputName: true\n      lowercaseOutputLabelNames: true\n      ssl: false\n      {{- if .Values.metrics.jmx.whitelistObjectNames }}\n      whitelistObjectNames: [\"{{ join \"\\\",\\\"\" .Values.metrics.jmx.whitelistObjectNames }}\"]\n      {{- end }}\n    ## @param metrics.jmx.existingConfigmap Name of existing ConfigMap with JMX exporter configuration\n    ## NOTE: This will override metrics.jmx.config\n    ##\n    existingConfigmap: \"\"\n  ## Prometheus Operator ServiceMonitor configuration\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled if `true`, creates a Prometheus Operator ServiceMonitor (requires `metrics.kafka.enabled` or `metrics.jmx.enabled` to be `true`)\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    interval: \"\"\n    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus\n    ##\n    labels: {}\n    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels\n    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration\n    ##\n    selector: {}\n    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping\n    ##\n    relabelings: []\n    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion\n    ##\n    metricRelabelings: []\n    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint\n    ##\n    honorLabels: false\n    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.\n    ##\n    jobLabel: \"\"\n\n## @section Kafka provisioning parameters\n\n## Kafka provisioning\n##\nprovisioning:\n  ## @param provisioning.enabled Enable kafka provisioning Job\n  ##\n  enabled: false\n  ## @param provisioning.numPartitions Default number of partitions for topics when unspecified\n  ##\n  numPartitions: 1\n  ## @param provisioning.replicationFactor Default replication factor for topics when unspecified\n  ##\n  replicationFactor: 3\n  ## @param provisioning.topics Kafka topics to provision\n  ## - name: topic-name\n  ##   partitions: 1\n  ##   replicationFactor: 1\n  ##   ## https://kafka.apache.org/documentation/#topicconfigs\n  ##   config:\n  ##     max.message.bytes: 64000\n  ##     flush.messages: 1\n  ##\n  topics: []\n  ## @param provisioning.tolerations Tolerations for pod assignment\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param provisioning.extraProvisioningCommands Extra commands to run to provision cluster resources\n  ## - echo \"Allow user to consume from any topic\"\n  ## - \u003e-\n  ##   /opt/bitnami/kafka/bin/kafka-acls.sh\n  ##   --bootstrap-server $KAFKA_SERVICE\n  ##   --command-config $CLIENT_CONF\n  ##   --add\n  ##   --allow-principal User:user\n  ##   --consumer --topic '*'\n  ## - \"/opt/bitnami/kafka/bin/kafka-acls.sh\n  ##      --bootstrap-server $KAFKA_SERVICE\n  ##      --command-config $CLIENT_CONF\n  ##      --list\"\n  ##\n  extraProvisioningCommands: []\n  ## @param provisioning.parallel Number of provisioning commands to run at the same time\n  ##\n  parallel: 1\n  ## @param provisioning.preScript Extra bash script to run before topic provisioning. $CLIENT_CONF is path to properties file with most needed configurations\n  ##\n  preScript: \"\"\n  ## @param provisioning.postScript Extra bash script to run after topic provisioning. $CLIENT_CONF is path to properties file with most needed configurations\n  ##\n  postScript: \"\"\n  ## Auth Configuration for kafka provisioning Job\n  ##\n  auth:\n    ## TLS configuration for kafka provisioning Job\n    ##\n    tls:\n      ## @param provisioning.auth.tls.type Format to use for TLS certificates. Allowed types: `jks` and `pem`.\n      ## Note: ignored if auth.tls.clientProtocol different from one of these values: \"tls\" \"mtls\" \"sasl_tls\".\n      ##\n      type: jks\n      ## @param provisioning.auth.tls.certificatesSecret Existing secret containing the TLS certificates for the Kafka provisioning Job.\n      ## When using 'jks' format for certificates, the secret should contain a truststore and a keystore.\n      ## When using 'pem' format for certificates, the secret should contain a public CA certificate, a public certificate and one private key.\n      ##\n      certificatesSecret: \"\"\n      ## @param provisioning.auth.tls.cert The secret key from the certificatesSecret if 'cert' key different from the default (tls.crt)\n      ##\n      cert: tls.crt\n      ## @param provisioning.auth.tls.key The secret key from the certificatesSecret if 'key' key different from the default (tls.key)\n      ##\n      key: tls.key\n      ## @param provisioning.auth.tls.caCert The secret key from the certificatesSecret if 'caCert' key different from the default (ca.crt)\n      ##\n      caCert: ca.crt\n      ## @param provisioning.auth.tls.keystore The secret key from the certificatesSecret if 'keystore' key different from the default (keystore.jks)\n      ##\n      keystore: keystore.jks\n      ## @param provisioning.auth.tls.truststore The secret key from the certificatesSecret if 'truststore' key different from the default (truststore.jks)\n      ##\n      truststore: truststore.jks\n      ## @param provisioning.auth.tls.passwordsSecret Name of the secret containing passwords to access the JKS files or PEM key when they are password-protected.\n      ## It should contain two keys called \"keystore-password\" and \"truststore-password\", or \"key-password\" if using a password-protected PEM key.\n      ##\n      passwordsSecret: \"\"\n      ## @param provisioning.auth.tls.keyPasswordSecretKey The secret key from the passwordsSecret if 'keyPasswordSecretKey' key different from the default (key-password)\n      ## Note: must not be used if `passwordsSecret` is not defined.\n      ##\n      keyPasswordSecretKey: key-password\n      ## @param provisioning.auth.tls.keystorePasswordSecretKey The secret key from the passwordsSecret if 'keystorePasswordSecretKey' key different from the default (keystore-password)\n      ## Note: must not be used if `passwordsSecret` is not defined.\n      ##\n      keystorePasswordSecretKey: keystore-password\n      ## @param provisioning.auth.tls.truststorePasswordSecretKey The secret key from the passwordsSecret if 'truststorePasswordSecretKey' key different from the default (truststore-password)\n      ## Note: must not be used if `passwordsSecret` is not defined.\n      ##\n      truststorePasswordSecretKey: truststore-password\n      ## @param provisioning.auth.tls.keyPassword Password to access the password-protected PEM key if necessary. Ignored if 'passwordsSecret' is provided.\n      ##\n      keyPassword: \"\"\n      ## @param provisioning.auth.tls.keystorePassword Password to access the JKS keystore. Ignored if 'passwordsSecret' is provided.\n      ##\n      keystorePassword: \"\"\n      ## @param provisioning.auth.tls.truststorePassword Password to access the JKS truststore. Ignored if 'passwordsSecret' is provided.\n      ##\n      truststorePassword: \"\"\n  ## @param provisioning.command Override provisioning container command\n  ##\n  command: []\n  ## @param provisioning.args Override provisioning container arguments\n  ##\n  args: []\n  ## @param provisioning.extraEnvVars Extra environment variables to add to the provisioning pod\n  ## e.g:\n  ## extraEnvVars:\n  ##   - name: KAFKA_CFG_BACKGROUND_THREADS\n  ##     value: \"10\"\n  ##\n  extraEnvVars: []\n  ## @param provisioning.extraEnvVarsCM ConfigMap with extra environment variables\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param provisioning.extraEnvVarsSecret Secret with extra environment variables\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param provisioning.podAnnotations Extra annotations for Kafka provisioning pods\n  ##\n  podAnnotations: {}\n  ## @param provisioning.podLabels Extra labels for Kafka provisioning pods\n  ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n  ##\n  podLabels: {}\n  ## Kafka provisioning resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param provisioning.resources.limits The resources limits for the Kafka provisioning container\n  ## @param provisioning.resources.requests The requested resources for the Kafka provisioning container\n  ##\n  resources:\n    limits: {}\n    requests: {}\n  ## Kafka provisioning pods' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod\n  ## @param provisioning.podSecurityContext.enabled Enable security context for the pods\n  ## @param provisioning.podSecurityContext.fsGroup Set Kafka provisioning pod's Security Context fsGroup\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## Kafka provisioning containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param provisioning.containerSecurityContext.enabled Enable Kafka provisioning containers' Security Context\n  ## @param provisioning.containerSecurityContext.runAsUser Set Kafka provisioning containers' Security Context runAsUser\n  ## @param provisioning.containerSecurityContext.runAsNonRoot Set Kafka provisioning containers' Security Context runAsNonRoot\n  ## e.g:\n  ##   containerSecurityContext:\n  ##     enabled: true\n  ##     capabilities:\n  ##       drop: [\"NET_RAW\"]\n  ##     readOnlyRootFilesystem: true\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n    runAsNonRoot: true\n  ## @param provisioning.schedulerName Name of the k8s scheduler (other than default) for kafka provisioning\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param provisioning.extraVolumes Optionally specify extra list of additional volumes for the Kafka provisioning pod(s)\n  ## e.g:\n  ## extraVolumes:\n  ##   - name: kafka-jaas\n  ##     secret:\n  ##       secretName: kafka-jaas\n  ##\n  extraVolumes: []\n  ## @param provisioning.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Kafka provisioning container(s)\n  ## extraVolumeMounts:\n  ##   - name: kafka-jaas\n  ##     mountPath: /bitnami/kafka/config/kafka_jaas.conf\n  ##     subPath: kafka_jaas.conf\n  ##\n  extraVolumeMounts: []\n  ## @param provisioning.sidecars Add additional sidecar containers to the Kafka provisioning pod(s)\n  ## e.g:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n  ## @param provisioning.initContainers Add additional Add init containers to the Kafka provisioning pod(s)\n  ## e.g:\n  ## initContainers:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  initContainers: []\n  ## @param provisioning.waitForKafka If true use an init container to wait until kafka is ready before starting provisioning\n  ##\n  waitForKafka: true\n\n## @section ZooKeeper chart parameters\n\n## ZooKeeper chart configuration\n## https://github.com/bitnami/charts/blob/master/bitnami/zookeeper/values.yaml\n##\nzookeeper:\n  ## @param zookeeper.enabled Switch to enable or disable the ZooKeeper helm chart\n  ##\n  enabled: true\n  ## @param zookeeper.replicaCount Number of ZooKeeper nodes\n  ##\n  replicaCount: 1\n  ## ZooKeeper authenticaiton\n  ##\n  auth:\n    client:\n      ## @param zookeeper.auth.client.enabled Enable ZooKeeper auth\n      ##\n      enabled: false\n      ## @param zookeeper.auth.client.clientUser User that will use ZooKeeper clients to auth\n      ##\n      clientUser: \"\"\n      ## @param zookeeper.auth.client.clientPassword Password that will use ZooKeeper clients to auth\n      ##\n      clientPassword: \"\"\n      ## @param zookeeper.auth.client.serverUsers Comma, semicolon or whitespace separated list of user to be created. Specify them as a string, for example: \"user1,user2,admin\"\n      ##\n      serverUsers: \"\"\n      ## @param zookeeper.auth.client.serverPasswords Comma, semicolon or whitespace separated list of passwords to assign to users when created. Specify them as a string, for example: \"pass4user1, pass4user2, pass4admin\"\n      ##\n      serverPasswords: \"\"\n  ## ZooKeeper Persistence parameters\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/\n  ## @param zookeeper.persistence.enabled Enable persistence on ZooKeeper using PVC(s)\n  ## @param zookeeper.persistence.storageClass Persistent Volume storage class\n  ## @param zookeeper.persistence.accessModes Persistent Volume access modes\n  ## @param zookeeper.persistence.size Persistent Volume size\n  ##\n  persistence:\n    enabled: true\n    storageClass: \"\"\n    accessModes:\n      - ReadWriteOnce\n    size: 2Gi\n\n## External Zookeeper Configuration\n## All of these values are only used if `zookeeper.enabled=false`\n##\nexternalZookeeper:\n  ## @param externalZookeeper.servers List of external zookeeper servers to use. Typically used in combination with 'zookeeperChrootPath'.\n  ##\n  servers: []\n"
            ],
            "verify": false,
            "version": "18.0.3",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kind_cluster.one-click"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "one_click_superset",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "status": "tainted",
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "../superset-helm",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "one-click-superset",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.0",
                "chart": "superset",
                "name": "one-click-superset",
                "namespace": "superset",
                "revision": 1,
                "values": "{\"adminUser\":{\"email\":\"admin@superset.com\",\"firstname\":\"Superset\",\"lastname\":\"Admin\",\"password\":\"admin123\",\"username\":\"admin\"},\"affinity\":{},\"bootstrapScript\":\"#!/bin/bash\\n#rm -rf /var/lib/apt/lists/* \\u0026\\u0026 \\\\\\npip install \\\\\\n  sqlalchemy==1.3.24 \\\\\\n  psycopg2==2.8.5 \\\\\\n  redis==3.2.1 \\u0026\\u0026 \\\\\\nif [ ! -f ~/bootstrap ]; then echo \\\"Running Superset with uid {{ .Values.runAsUser }}\\\" \\u003e ~/bootstrap; fi\\n\",\"configFromSecret\":\"{{ template \\\"superset.fullname\\\" . }}-config\",\"configMountPath\":\"/etc/superset\",\"configOverrides\":{\"data_cache_config\":\"DATA_CACHE_CONFIG = {\\n  'CACHE_TYPE': 'redis',\\n  'CACHE_DEFAULT_TIMEOUT': 600,\\n  'CACHE_KEY_PREFIX': 'superset_',\\n  'CACHE_REDIS_URL': 'redis://{{ template \\\"superset.fullname\\\" . }}-redis-headless:6379/1'\\n}\\n\",\"enable_feature_flags\":\"FEATURE_FLAGS = {\\n  \\\"DASHBOARD_NATIVE_FILTERS\\\": True,\\n  \\\"DASHBOARD_CROSS_FILTERS\\\": True,\\n  \\\"DASHBOARD_NATIVE_FILTERS_SET\\\": True,\\n  \\\"ENABLE_TEMPLATE_PROCESSING\\\": True,\\n}\\n\",\"oauth\":\"from flask_appbuilder.security.manager import (AUTH_DB, AUTH_OAUTH)\\nAUTH_TYPE = AUTH_OAUTH\\n\\nOAUTH_PROVIDERS = [\\n  {\\n      \\\"name\\\": \\\"google\\\",\\n      \\\"whitelist\\\": [ \\\"{{ .Values.oauth.email_whitelist_regex }}\\\" ],\\n      \\\"icon\\\": \\\"fa-google\\\",\\n      \\\"token_key\\\": \\\"access_token\\\",\\n      \\\"remote_app\\\": {\\n          \\\"client_id\\\": \\\"{{ .Values.oauth.client_id }}\\\",\\n          \\\"client_secret\\\": \\\"{{ .Values.oauth.client_secret }}\\\",\\n          \\\"api_base_url\\\": \\\"https://www.googleapis.com/oauth2/v2/\\\",\\n          \\\"client_kwargs\\\": {\\\"scope\\\": \\\"email profile\\\"},\\n          \\\"request_token_url\\\": None,\\n          \\\"access_token_url\\\": \\\"https://accounts.google.com/o/oauth2/token\\\",\\n          \\\"authorize_url\\\": \\\"https://accounts.google.com/o/oauth2/auth\\\",\\n          \\\"authorize_params\\\": {\\\"hd\\\": \\\"{{ .Values.oauth.whitelist_domain }}\\\"}\\n      }\\n  }\\n]\\n# Map Authlib roles to superset roles\\nAUTH_ROLE_ADMIN = 'Admin'\\nAUTH_ROLE_PUBLIC = 'Public'\\n# Will allow user self registration, allowing to create Flask users from Authorized User\\nAUTH_USER_REGISTRATION = True\\n# The default user self registration role\\nAUTH_USER_REGISTRATION_ROLE = \\\"{{ oauth.user_registration_role }}\\\"\\n\",\"sql_alchemy_config\":\"SQLALCHEMY_DATABASE_URI = 'postgresql://{{ tpl .Values.postgres.superset.db_username . }}:{{ tpl .Values.postgres.superset.db_password . }}@{{ template \\\"superset.fullname\\\" . }}-postgresql-headless:5432/{{ tpl .Values.postgres.superset.db_name . }}'\\nSQLALCHEMY_TRACK_MODIFICATIONS = True\\nSECRET_KEY = 'thisISaSECRET_1234'\\n\"},\"envFromSecret\":\"{{ template \\\"superset.fullname\\\" . }}-env\",\"envFromSecrets\":[],\"extraConfigMountPath\":\"/app/configs\",\"extraConfigs\":{},\"extraEnv\":{},\"extraSecretEnv\":{},\"extraSecrets\":{},\"image\":{\"pullPolicy\":\"Always\",\"repository\":\"amancevice/superset\",\"tag\":\"1.2.0\"},\"imagePullSecrets\":[],\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[\"chart-example.local\"],\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"tls\":[]},\"init\":{\"command\":[\"/bin/sh\",\"-c\",\". {{ .Values.configMountPath }}/superset_bootstrap.sh; . {{ .Values.configMountPath }}/superset_init.sh\"],\"createAdmin\":true,\"enabled\":true,\"initscript\":\"#!/bin/sh\\necho \\\"Upgrading DB schema...\\\"\\nsuperset db upgrade\\necho \\\"Initializing roles...\\\"\\nsuperset init\\n{{ if .Values.init.createAdmin }}\\necho \\\"Creating admin user...\\\"\\nsuperset fab create-admin \\\\\\n                --username {{ .Values.adminUser.username }} \\\\\\n                --firstname {{ .Values.adminUser.firstname }} \\\\\\n                --lastname {{ .Values.adminUser.lastname }} \\\\\\n                --email {{ .Values.adminUser.email }} \\\\\\n                --password {{ .Values.adminUser.password }} \\\\\\n                || true\\n{{ end }}\\nif [ -f \\\"{{ .Values.extraConfigMountPath }}/import_datasources.yaml\\\" ]; then\\n  echo \\\"Importing database connections.... \\\"\\n  superset import_datasources -p {{ .Values.extraConfigMountPath }}/import_datasources.yaml\\nfi\",\"resources\":{}},\"nodeSelector\":{},\"oauth\":{\"client_id\":\"client_id\",\"client_secret\":\"client_secret\",\"email_whitelist_regex\":\"\",\"enabled\":false,\"user_registration_role\":\"Gamma\",\"whitelist_domain\":\"\"},\"oauth_enabled\":false,\"postgres\":{\"adminPassword\":\"postgres\",\"adminUser\":\"postgres\",\"db_port\":5432,\"superset\":{\"db_name\":\"superset\",\"db_password\":\"superset$123\",\"db_username\":\"superset\"}},\"postgresql\":{\"enabled\":true,\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":false},\"postgresqlDatabase\":\"superset\",\"postgresqlPassword\":\"superset$123\",\"postgresqlPostgresPassword\":\"postgres\",\"postgresqlUsername\":\"superset\",\"service\":{\"port\":5432}},\"redis\":{\"cluster\":{\"enabled\":false},\"enabled\":true,\"master\":{\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"enabled\":false}},\"usePassword\":false},\"replicaCount\":1,\"resources\":{},\"runAsUser\":1000,\"service\":{\"annotations\":{},\"port\":8088,\"type\":\"LoadBalancer\"},\"supersetNode\":{\"connections\":{\"db_host\":\"{{ template \\\"superset.fullname\\\" . }}-postgres-headless\",\"db_name\":\"{{ .Values.postgres.superset.db_name }}\",\"db_pass\":\"{{ .Values.postgres.superset.db_password }}\",\"db_port\":\"{{ .Values.postgres.db_port }}\",\"db_user\":\"{{ .Values.postgres.superset.db_username }}\",\"redis_host\":\"{{ template \\\"superset.fullname\\\" . }}-redis-headless\",\"redis_port\":\"6379\"},\"deploymentAnnotations\":{},\"forceReload\":false,\"podAnnotations\":{}},\"tolerations\":[]}",
                "version": "0.3.5"
              }
            ],
            "name": "one-click-superset",
            "namespace": "superset",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "failed",
            "timeout": 300,
            "values": [
              "# global:\npostgres:\n  adminUser: \"postgres\"\n  adminPassword: \"postgres\"\n  db_port: 5432\n  superset:\n    db_name: \"superset\"\n    db_username: \"superset\"\n    db_password: \"superset$123\"\n\nreplicaCount: 1\noauth_enabled: False\n\nadminUser:\n  username: \"admin\"\n  firstname: \"Superset\"\n  lastname: \"Admin\"\n  email: \"admin@superset.com\"\n  password: \"admin123\"\n\noauth:\n  enabled: false\n  client_id: \"client_id\"\n  client_secret: \"client_secret\"\n  email_whitelist_regex: \"\"\n  whitelist_domain: \"\"\n  user_registration_role: \"Gamma\"\n\nrunAsUser: 1000\n\n# Install additional packages and do any other bootstrap configuration in this script\n# For production clusters it's recommended to build own image with this step done in CI\nbootstrapScript: |\n  #!/bin/bash\n  #rm -rf /var/lib/apt/lists/* \u0026\u0026 \\\n  pip install \\\n    sqlalchemy==1.3.24 \\\n    psycopg2==2.8.5 \\\n    redis==3.2.1 \u0026\u0026 \\\n  if [ ! -f ~/bootstrap ]; then echo \"Running Superset with uid {{ .Values.runAsUser }}\" \u003e ~/bootstrap; fi\n\n## The name of the secret which we will use to generate a superset_config.py file\n## Note: this secret must have the key superset_config.py in it and can include other files as well\n##\nconfigFromSecret: '{{ template \"superset.fullname\" . }}-config'\n\n## The name of the secret which we will use to populate env vars in deployed pods\n## This can be useful for secret keys, etc.\n##\nenvFromSecret: '{{ template \"superset.fullname\" . }}-env'\n## This can be a list of template strings\nenvFromSecrets: []\n\n## Extra environment variables that will be passed into pods\n##\nextraEnv: {}\n  # Extend timeout to allow long running queries.\n  # GUNICORN_TIMEOUT: 300\n\n\n   # OAUTH_HOME_DOMAIN: ..\n  # # If a whitelist is not set, any address that can use your OAuth2 endpoint will be able to login.\n  # #   this includes any random Gmail address if your OAuth2 Web App is set to External.\n  # OAUTH_WHITELIST_REGEX: ...\n\n## Extra environment variables to pass as secrets\n##\nextraSecretEnv: {}\n  # MAPBOX_API_KEY: ...\n  # # Google API Keys: https://console.cloud.google.com/apis/credentials\n  # GOOGLE_KEY: ...\n  # GOOGLE_SECRET: ...\n\nextraConfigs: {}\n  # datasources-init.yaml: |\n  #     databases:\n  #     - allow_csv_upload: true\n  #       allow_ctas: true\n  #       allow_cvas: true\n  #       database_name: example-db\n  #       extra: \"{\\r\\n    \\\"metadata_params\\\": {},\\r\\n    \\\"engine_params\\\": {},\\r\\n    \\\"\\\n  #         metadata_cache_timeout\\\": {},\\r\\n    \\\"schemas_allowed_for_csv_upload\\\": []\\r\\n\\\n  #         }\"\n  #       sqlalchemy_uri: example://example-db.local\n  #       tables: []\n\nextraSecrets: {}\n\n# A dictionary of overrides to append at the end of superset_config.py - the name does not matter\n# WARNING: the order is not guaranteed\nconfigOverrides:\n  enable_feature_flags: |\n    FEATURE_FLAGS = {\n      \"DASHBOARD_NATIVE_FILTERS\": True,\n      \"DASHBOARD_CROSS_FILTERS\": True,\n      \"DASHBOARD_NATIVE_FILTERS_SET\": True,\n      \"ENABLE_TEMPLATE_PROCESSING\": True,\n    }\n\n  data_cache_config: |\n    DATA_CACHE_CONFIG = {\n      'CACHE_TYPE': 'redis',\n      'CACHE_DEFAULT_TIMEOUT': 600,\n      'CACHE_KEY_PREFIX': 'superset_',\n      'CACHE_REDIS_URL': 'redis://{{ template \"superset.fullname\" . }}-redis-headless:6379/1'\n    }\n\n  sql_alchemy_config: |\n    SQLALCHEMY_DATABASE_URI = 'postgresql://{{ tpl .Values.postgres.superset.db_username . }}:{{ tpl .Values.postgres.superset.db_password . }}@{{ template \"superset.fullname\" . }}-postgresql-headless:5432/{{ tpl .Values.postgres.superset.db_name . }}'\n    SQLALCHEMY_TRACK_MODIFICATIONS = True\n    SECRET_KEY = 'thisISaSECRET_1234'\n\n  #map_box_key: |\n  #  MAPBOX_API_KEY=''\n\n  oauth: |\n    from flask_appbuilder.security.manager import (AUTH_DB, AUTH_OAUTH)\n    AUTH_TYPE = AUTH_OAUTH\n\n    OAUTH_PROVIDERS = [\n      {\n          \"name\": \"google\",\n          \"whitelist\": [ \"{{ .Values.oauth.email_whitelist_regex }}\" ],\n          \"icon\": \"fa-google\",\n          \"token_key\": \"access_token\",\n          \"remote_app\": {\n              \"client_id\": \"{{ .Values.oauth.client_id }}\",\n              \"client_secret\": \"{{ .Values.oauth.client_secret }}\",\n              \"api_base_url\": \"https://www.googleapis.com/oauth2/v2/\",\n              \"client_kwargs\": {\"scope\": \"email profile\"},\n              \"request_token_url\": None,\n              \"access_token_url\": \"https://accounts.google.com/o/oauth2/token\",\n              \"authorize_url\": \"https://accounts.google.com/o/oauth2/auth\",\n              \"authorize_params\": {\"hd\": \"{{ .Values.oauth.whitelist_domain }}\"}\n          }\n      }\n    ]\n    # Map Authlib roles to superset roles\n    AUTH_ROLE_ADMIN = 'Admin'\n    AUTH_ROLE_PUBLIC = 'Public'\n    # Will allow user self registration, allowing to create Flask users from Authorized User\n    AUTH_USER_REGISTRATION = True\n    # The default user self registration role\n    AUTH_USER_REGISTRATION_ROLE = \"{{ oauth.user_registration_role }}\"\n\nconfigMountPath: \"/etc/superset\"\n\nextraConfigMountPath: \"/app/configs\"\n\nimage:\n  repository: amancevice/superset\n  tag: 1.2.0\n  pullPolicy: Always\n\nimagePullSecrets: []\n\n\nservice:\n  type: LoadBalancer \n  port: 8088\n  annotations: {}\n\ningress:\n  enabled: false\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n    ## Extend timeout to allow long running queries.\n    # nginx.ingress.kubernetes.io/proxy-connect-timeout: \"300\"\n    # nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    # nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n  path: /\n  pathType: ImplementationSpecific\n  hosts:\n    - chart-example.local\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources: {}\n  # We usually recommend not to specify default resources and to leave this as a conscious\n  # choice for the user. This also increases chances charts run on environments with little\n  # resources, such as Minikube. If you do want to specify resources, uncomment the following\n  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  # limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n##\n## Superset node configuration\nsupersetNode:\n  connections:\n    redis_host: '{{ template \"superset.fullname\" . }}-redis-headless'\n    redis_port: \"6379\"\n    db_host: '{{ template \"superset.fullname\" . }}-postgres-headless'\n    db_port: \"{{ .Values.postgres.db_port }}\"\n    db_user: \"{{ .Values.postgres.superset.db_username }}\"\n    db_pass: \"{{ .Values.postgres.superset.db_password }}\"\n    db_name: \"{{ .Values.postgres.superset.db_name }}\"\n  forceReload: false # If true, forces deployment to reload on each upgrade\n  # initContainers:\n  #   - name: wait-for-postgres\n  #     image: busybox:latest\n  #     imagePullPolicy: IfNotPresent\n  #     envFrom:\n  #       - secretRef:\n  #           name: '{{ tpl .Values.envFromSecret . }}'\n  #     command: [ \"/bin/sh\", \"-c\", \"until nc -zv $DB_HOST $DB_PORT -w1; do echo 'waiting for db'; sleep 1; done\" ]\n  ## Annotations to be added to supersetNode deployment\n  deploymentAnnotations: {}\n  ## Annotations to be added to supersetNode pods\n  podAnnotations: {}\n\n##\n## Init job configuration\ninit:\n  # Configure resources\n  # Warning: fab command consumes a lot of ram and can\n  # cause the process to be killed due to OOM if it exceeds limit\n  resources: {}\n    # limits:\n    #   cpu:\n    #   memory:\n    # requests:\n    #   cpu:\n    #   memory:\n  command:\n    - \"/bin/sh\"\n    - \"-c\"\n    - \". {{ .Values.configMountPath }}/superset_bootstrap.sh; . {{ .Values.configMountPath }}/superset_init.sh\"\n  enabled: true\n  createAdmin: true\n  # initContainers:\n  #   - name: wait-for-postgres\n  #     image: busybox:latest\n  #     imagePullPolicy: IfNotPresent\n  #     envFrom:\n  #       - secretRef:\n  #           name: '{{ tpl .Values.envFromSecret . }}'\n  #     command: [ \"/bin/sh\", \"-c\", \"until nc -zv $DB_HOST $DB_PORT -w1; do echo 'waiting for db'; sleep 1; done\" ]\n  initscript: |-\n    #!/bin/sh\n    echo \"Upgrading DB schema...\"\n    superset db upgrade\n    echo \"Initializing roles...\"\n    superset init\n    {{ if .Values.init.createAdmin }}\n    echo \"Creating admin user...\"\n    superset fab create-admin \\\n                    --username {{ .Values.adminUser.username }} \\\n                    --firstname {{ .Values.adminUser.firstname }} \\\n                    --lastname {{ .Values.adminUser.lastname }} \\\n                    --email {{ .Values.adminUser.email }} \\\n                    --password {{ .Values.adminUser.password }} \\\n                    || true\n    {{ end }}\n    if [ -f \"{{ .Values.extraConfigMountPath }}/import_datasources.yaml\" ]; then\n      echo \"Importing database connections.... \"\n      superset import_datasources -p {{ .Values.extraConfigMountPath }}/import_datasources.yaml\n    fi\n\n## Configuration values for the Redis dependency.\n## ref: https://github.com/kubernetes/charts/blob/master/stable/redis/README.md\nredis:\n  ##\n  ## Use the redis chart dependency.\n  ## Set to false if bringing your own redis.\n  enabled: true\n  usePassword: false\n  ##\n  ## If you are bringing your own redis, you can set the host in redisHost.\n  ## redisHost:\n  ##\n  ## Redis password\n  ##\n  ## password: superset\n  ##\n  ## Master configuration\n  master:\n    ##\n    ## Image configuration\n    # image:\n      ##\n      ## docker registry secret names (list)\n      # pullSecrets: nil\n    ##\n    ## Configure persistance\n    persistence:\n      ##\n      ## Use a PVC to persist data.\n      enabled: false\n      ##\n      ## Persistant class\n      # storageClass: classname\n      ##\n      ## Access mode:\n      accessModes:\n      - ReadWriteOnce\n  ##\n  ## Disable cluster management by default.\n  cluster:\n    enabled: false\n\n##\n## Configuration values for the postgresql dependency.\n## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md\npostgresql:\n  ##\n  ## Use the PostgreSQL chart dependency.\n  ## Set to false if bringing your own PostgreSQL.\n  enabled: true\n  ##\n  ## If you are bringing your own PostgreSQL, you should set postgresHost and\n  ## also probably service.port, postgresqlUsername, postgresqlPassword, and postgresqlDatabase\n  ## postgresHost: \"{{ postgres.db_url }}\"\n  ##\n  ## PostgreSQL port\n  service:\n    port: 5432\n\n  postgresqlPostgresPassword: \"postgres\"\n  ## PostgreSQL User to create.\n  postgresqlUsername: \"superset\"\n  ##\n  ## PostgreSQL Password for the new user.\n  ## If not set, a random 10 characters password will be used.\n  postgresqlPassword: \"superset$123\"\n  ##\n  ## PostgreSQL Database to create.\n  postgresqlDatabase: \"superset\"\n  ##\n  ## Persistent Volume Storage configuration.\n  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes\n  persistence:\n    ##\n    ## Enable PostgreSQL persistence using Persistent Volume Claims.\n    enabled: false\n    ##\n    ## Persistant class\n    # storageClass: classname\n    ##\n    ## Access modes:\n    accessModes:\n      - ReadWriteOnce\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n"
            ],
            "verify": false,
            "version": "0.3.5",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "helm_release.druid_cluster",
            "helm_release.druid_operator",
            "helm_release.postgres",
            "kind_cluster.one-click"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "postgres",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "../postgresql",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "postgres",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "14.4.0",
                "chart": "postgresql",
                "name": "postgres",
                "namespace": "postgres",
                "revision": 1,
                "values": "{\"architecture\":\"standalone\",\"audit\":{\"clientMinMessages\":\"error\",\"logConnections\":false,\"logDisconnections\":false,\"logHostname\":false,\"logLinePrefix\":\"\",\"logTimezone\":\"\",\"pgAuditLog\":\"\",\"pgAuditLogCatalog\":\"off\"},\"auth\":{\"database\":\"druiddb\",\"enablePostgresUser\":true,\"existingSecret\":\"\",\"password\":\"druid\",\"postgresPassword\":\"postgres\",\"replicationPassword\":\"\",\"replicationUsername\":\"repl_user\",\"secretKeys\":{\"adminPasswordKey\":\"postgres-password\",\"replicationPasswordKey\":\"replication-password\",\"userPasswordKey\":\"password\"},\"usePasswordFiles\":false,\"username\":\"druid\"},\"clusterDomain\":\"cluster.local\",\"commonAnnotations\":{},\"commonLabels\":{},\"containerPorts\":{\"postgresql\":5432},\"diagnosticMode\":{\"args\":[\"infinity\"],\"command\":[\"sleep\"],\"enabled\":false},\"extraDeploy\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"postgresql\":{\"auth\":{\"database\":\"druiddb\",\"existingSecret\":\"\",\"password\":\"druid\",\"postgresPassword\":\"postgres\",\"secretKeys\":{\"adminPasswordKey\":\"\",\"replicationPasswordKey\":\"\",\"userPasswordKey\":\"\"},\"username\":\"druid\"},\"service\":{\"ports\":{\"postgresql\":\"\"}}},\"storageClass\":\"\"},\"image\":{\"debug\":false,\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/postgresql\",\"tag\":\"14.4.0-debian-11-r13\"},\"kubeVersion\":\"\",\"ldap\":{\"basedn\":\"\",\"binddn\":\"\",\"bindpw\":\"\",\"enabled\":false,\"port\":\"\",\"prefix\":\"\",\"scheme\":\"\",\"searchAttribute\":\"\",\"searchFilter\":\"\",\"server\":\"\",\"suffix\":\"\",\"tls\":{\"enabled\":false},\"uri\":\"\"},\"metrics\":{\"containerPorts\":{\"metrics\":9187},\"containerSecurityContext\":{\"enabled\":true,\"runAsNonRoot\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customMetrics\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"enabled\":false,\"extraEnvVars\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/postgres-exporter\",\"tag\":\"0.10.1-debian-11-r18\"},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"prometheusRule\":{\"enabled\":false,\"labels\":{},\"namespace\":\"\",\"rules\":[]},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"resources\":{\"limits\":{},\"requests\":{}},\"service\":{\"annotations\":{\"prometheus.io/port\":\"{{ .Values.metrics.service.ports.metrics }}\",\"prometheus.io/scrape\":\"true\"},\"clusterIP\":\"\",\"ports\":{\"metrics\":9187},\"sessionAffinity\":\"None\"},\"serviceMonitor\":{\"enabled\":false,\"honorLabels\":false,\"interval\":\"\",\"jobLabel\":\"\",\"labels\":{},\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scrapeTimeout\":\"\",\"selector\":{}},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":15,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}},\"nameOverride\":\"\",\"networkPolicy\":{\"egressRules\":{\"customRules\":{},\"denyConnectionsToExternal\":false},\"enabled\":false,\"ingressRules\":{\"primaryAccessOnlyFrom\":{\"customRules\":{},\"enabled\":false,\"namespaceSelector\":{},\"podSelector\":{}},\"readReplicasAccessOnlyFrom\":{\"customRules\":{},\"enabled\":false,\"namespaceSelector\":{},\"podSelector\":{}}},\"metrics\":{\"enabled\":false,\"namespaceSelector\":{},\"podSelector\":{}}},\"postgresqlDataDir\":\"/bitnami/postgresql/data\",\"postgresqlSharedPreloadLibraries\":\"pgaudit\",\"primary\":{\"affinity\":{},\"annotations\":{},\"args\":[],\"command\":[],\"configuration\":\"\",\"containerSecurityContext\":{\"enabled\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"existingConfigmap\":\"\",\"existingExtendedConfigmap\":\"\",\"extendedConfiguration\":\"\",\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraPodSpec\":{},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostIPC\":false,\"hostNetwork\":false,\"initContainers\":[],\"initdb\":{\"args\":\"\",\"password\":\"\",\"postgresqlWalDir\":\"\",\"scripts\":{},\"scriptsConfigMap\":\"\",\"scriptsSecret\":\"\",\"user\":\"\"},\"labels\":{},\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"dataSource\":{},\"enabled\":true,\"existingClaim\":\"\",\"mountPath\":\"/bitnami/postgresql\",\"selector\":{},\"size\":\"8Gi\",\"storageClass\":\"\",\"subPath\":\"\"},\"pgHbaConfiguration\":\"\",\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"resources\":{\"limits\":{},\"requests\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"}},\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"extraPorts\":[],\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"postgresql\":\"\"},\"ports\":{\"postgresql\":5432},\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{},\"type\":\"ClusterIP\"},\"sidecars\":[],\"standby\":{\"enabled\":false,\"primaryHost\":\"\",\"primaryPort\":\"\"},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":15,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"terminationGracePeriodSeconds\":\"\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":{\"rollingUpdate\":{},\"type\":\"RollingUpdate\"}},\"psp\":{\"create\":false},\"rbac\":{\"create\":false,\"rules\":[]},\"readReplicas\":{\"affinity\":{},\"annotations\":{},\"args\":[],\"command\":[],\"containerSecurityContext\":{\"enabled\":true,\"runAsUser\":1001},\"customLivenessProbe\":{},\"customReadinessProbe\":{},\"customStartupProbe\":{},\"extraEnvVars\":[],\"extraEnvVarsCM\":\"\",\"extraEnvVarsSecret\":\"\",\"extraPodSpec\":{},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"hostIPC\":false,\"hostNetwork\":false,\"initContainers\":[],\"labels\":{},\"lifecycleHooks\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"nodeAffinityPreset\":{\"key\":\"\",\"type\":\"\",\"values\":[]},\"nodeSelector\":{},\"persistence\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"dataSource\":{},\"enabled\":true,\"mountPath\":\"/bitnami/postgresql\",\"selector\":{},\"size\":\"8Gi\",\"storageClass\":\"\",\"subPath\":\"\"},\"podAffinityPreset\":\"\",\"podAnnotations\":{},\"podAntiAffinityPreset\":\"soft\",\"podLabels\":{},\"podSecurityContext\":{\"enabled\":true,\"fsGroup\":1001},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":6,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"replicaCount\":1,\"resources\":{\"limits\":{},\"requests\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"}},\"schedulerName\":\"\",\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalTrafficPolicy\":\"Cluster\",\"extraPorts\":[],\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePorts\":{\"postgresql\":\"\"},\"ports\":{\"postgresql\":5432},\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{},\"type\":\"ClusterIP\"},\"sidecars\":[],\"startupProbe\":{\"enabled\":false,\"failureThreshold\":15,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"terminationGracePeriodSeconds\":\"\",\"tolerations\":[],\"topologySpreadConstraints\":[],\"updateStrategy\":{\"rollingUpdate\":{},\"type\":\"RollingUpdate\"}},\"replication\":{\"applicationName\":\"my_application\",\"numSynchronousReplicas\":0,\"synchronousCommit\":\"off\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"name\":\"\"},\"shmVolume\":{\"enabled\":true,\"sizeLimit\":\"\"},\"tls\":{\"autoGenerated\":false,\"certCAFilename\":\"\",\"certFilename\":\"\",\"certKeyFilename\":\"\",\"certificatesSecret\":\"\",\"crlFilename\":\"\",\"enabled\":false,\"preferServerCiphers\":true},\"volumePermissions\":{\"containerSecurityContext\":{\"runAsUser\":0},\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"registry\":\"docker.io\",\"repository\":\"bitnami/bitnami-shell\",\"tag\":\"11-debian-11-r17\"},\"resources\":{\"limits\":{},\"requests\":{}}}}",
                "version": "11.6.19"
              }
            ],
            "name": "postgres",
            "namespace": "postgres",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "## @section Global parameters\n## Please, note that this will override the parameters, including dependencies, configured to use the global value\n##\nglobal:\n  ## @param global.imageRegistry Global Docker image registry\n  ##\n  imageRegistry: \"\"\n  ## @param global.imagePullSecrets Global Docker registry secret names as an array\n  ## e.g.\n  ## imagePullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  imagePullSecrets: []\n  ## @param global.storageClass Global StorageClass for Persistent Volume(s)\n  ##\n  storageClass: \"\"\n  postgresql:\n    ## @param global.postgresql.auth.postgresPassword Password for the \"postgres\" admin user (overrides `auth.postgresPassword`)\n    ## @param global.postgresql.auth.username Name for a custom user to create (overrides `auth.username`)\n    ## @param global.postgresql.auth.password Password for the custom user to create (overrides `auth.password`)\n    ## @param global.postgresql.auth.database Name for a custom database to create (overrides `auth.database`)\n    ## @param global.postgresql.auth.existingSecret Name of existing secret to use for PostgreSQL credentials (overrides `auth.existingSecret`).\n    ## @param global.postgresql.auth.secretKeys.adminPasswordKey Name of key in existing secret to use for PostgreSQL credentials (overrides `auth.secretKeys.adminPasswordKey`). Only used when `global.postgresql.auth.existingSecret` is set.\n    ## @param global.postgresql.auth.secretKeys.userPasswordKey Name of key in existing secret to use for PostgreSQL credentials (overrides `auth.secretKeys.userPasswordKey`). Only used when `global.postgresql.auth.existingSecret` is set.\n    ## @param global.postgresql.auth.secretKeys.replicationPasswordKey Name of key in existing secret to use for PostgreSQL credentials (overrides `auth.secretKeys.replicationPasswordKey`). Only used when `global.postgresql.auth.existingSecret` is set.\n    ##\n    auth:\n      postgresPassword: \"postgres\"\n      username: \"druid\"\n      password: \"druid\"\n      database: \"druiddb\"\n      existingSecret: \"\"\n      secretKeys:\n        adminPasswordKey: \"\"\n        userPasswordKey: \"\"\n        replicationPasswordKey: \"\"\n    ## @param global.postgresql.service.ports.postgresql PostgreSQL service port (overrides `service.ports.postgresql`)\n    ##\n    service:\n      ports:\n        postgresql: \"\"\n\n## @section Common parameters\n##\n\n## @param kubeVersion Override Kubernetes version\n##\nkubeVersion: \"\"\n## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)\n##\nnameOverride: \"\"\n## @param fullnameOverride String to fully override common.names.fullname template\n##\nfullnameOverride: \"\"\n## @param clusterDomain Kubernetes Cluster Domain\n##\nclusterDomain: cluster.local\n## @param extraDeploy Array of extra objects to deploy with the release (evaluated as a template)\n##\nextraDeploy: []\n## @param commonLabels Add labels to all the deployed resources\n##\ncommonLabels: {}\n## @param commonAnnotations Add annotations to all the deployed resources\n##\ncommonAnnotations: {}\n## Enable diagnostic mode in the statefulset\n##\ndiagnosticMode:\n  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)\n  ##\n  enabled: false\n  ## @param diagnosticMode.command Command to override all containers in the statefulset\n  ##\n  command:\n    - sleep\n  ## @param diagnosticMode.args Args to override all containers in the statefulset\n  ##\n  args:\n    - infinity\n\n## @section PostgreSQL common parameters\n##\n\n## Bitnami PostgreSQL image version\n## ref: https://hub.docker.com/r/bitnami/postgresql/tags/\n## @param image.registry PostgreSQL image registry\n## @param image.repository PostgreSQL image repository\n## @param image.tag PostgreSQL image tag (immutable tags are recommended)\n## @param image.pullPolicy PostgreSQL image pull policy\n## @param image.pullSecrets Specify image pull secrets\n## @param image.debug Specify if debug values should be set\n##\nimage:\n  registry: docker.io\n  repository: bitnami/postgresql\n  tag: 14.4.0-debian-11-r13\n  ## Specify a imagePullPolicy\n  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'\n  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images\n  ##\n  pullPolicy: IfNotPresent\n  ## Optionally specify an array of imagePullSecrets.\n  ## Secrets must be manually created in the namespace.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ## Example:\n  ## pullSecrets:\n  ##   - myRegistryKeySecretName\n  ##\n  pullSecrets: []\n  ## Set to true if you would like to see extra information on logs\n  ##\n  debug: false\n## Authentication parameters\n## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#setting-the-root-password-on-first-run\n## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-on-first-run\n## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#creating-a-database-user-on-first-run\n##\nauth:\n  ## @param auth.enablePostgresUser Assign a password to the \"postgres\" admin user. Otherwise, remote access will be blocked for this user\n  ##\n  enablePostgresUser: true\n  ## @param auth.postgresPassword Password for the \"postgres\" admin user. Ignored if `auth.existingSecret` with key `postgres-password` is provided\n  ##\n  postgresPassword: \"postgres\"\n  ## @param auth.username Name for a custom user to create\n  ##\n  username: \"druid\"\n  ## @param auth.password Password for the custom user to create. Ignored if `auth.existingSecret` with key `password` is provided\n  ##\n  password: \"druid\"\n  ## @param auth.database Name for a custom database to create\n  ##\n  database: \"druiddb\"\n  ## @param auth.replicationUsername Name of the replication user\n  ##\n  replicationUsername: repl_user\n  ## @param auth.replicationPassword Password for the replication user. Ignored if `auth.existingSecret` with key `replication-password` is provided\n  ##\n  replicationPassword: \"\"\n  ## @param auth.existingSecret Name of existing secret to use for PostgreSQL credentials. `auth.postgresPassword`, `auth.password`, and `auth.replicationPassword` will be ignored and picked up from this secret. The secret might also contains the key `ldap-password` if LDAP is enabled. `ldap.bind_password` will be ignored and picked from this secret in this case.\n  ##\n  existingSecret: \"\"\n  ## @param auth.secretKeys.adminPasswordKey Name of key in existing secret to use for PostgreSQL credentials. Only used when `auth.existingSecret` is set.\n  ## @param auth.secretKeys.userPasswordKey Name of key in existing secret to use for PostgreSQL credentials. Only used when `auth.existingSecret` is set.\n  ## @param auth.secretKeys.replicationPasswordKey Name of key in existing secret to use for PostgreSQL credentials. Only used when `auth.existingSecret` is set.\n  ##\n  secretKeys:\n    adminPasswordKey: postgres-password\n    userPasswordKey: password\n    replicationPasswordKey: replication-password\n  ## @param auth.usePasswordFiles Mount credentials as a files instead of using an environment variable\n  ##\n  usePasswordFiles: false\n## @param architecture PostgreSQL architecture (`standalone` or `replication`)\n##\narchitecture: standalone\n## Replication configuration\n## Ignored if `architecture` is `standalone`\n##\nreplication:\n  ## @param replication.synchronousCommit Set synchronous commit mode. Allowed values: `on`, `remote_apply`, `remote_write`, `local` and `off`\n  ## @param replication.numSynchronousReplicas Number of replicas that will have synchronous replication. Note: Cannot be greater than `readReplicas.replicaCount`.\n  ## ref: https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT\n  ##\n  synchronousCommit: \"off\"\n  numSynchronousReplicas: 0\n  ## @param replication.applicationName Cluster application name. Useful for advanced replication settings\n  ##\n  applicationName: my_application\n## @param containerPorts.postgresql PostgreSQL container port\n##\ncontainerPorts:\n  postgresql: 5432\n## Audit settings\n## https://github.com/bitnami/bitnami-docker-postgresql#auditing\n## @param audit.logHostname Log client hostnames\n## @param audit.logConnections Add client log-in operations to the log file\n## @param audit.logDisconnections Add client log-outs operations to the log file\n## @param audit.pgAuditLog Add operations to log using the pgAudit extension\n## @param audit.pgAuditLogCatalog Log catalog using pgAudit\n## @param audit.clientMinMessages Message log level to share with the user\n## @param audit.logLinePrefix Template for log line prefix (default if not set)\n## @param audit.logTimezone Timezone for the log timestamps\n##\naudit:\n  logHostname: false\n  logConnections: false\n  logDisconnections: false\n  pgAuditLog: \"\"\n  pgAuditLogCatalog: \"off\"\n  clientMinMessages: error\n  logLinePrefix: \"\"\n  logTimezone: \"\"\n## LDAP configuration\n## @param ldap.enabled Enable LDAP support\n## DEPRECATED ldap.url It will removed in a future, please use 'ldap.uri' instead\n## @param ldap.server IP address or name of the LDAP server.\n## @param ldap.port Port number on the LDAP server to connect to\n## @param ldap.prefix String to prepend to the user name when forming the DN to bind\n## @param ldap.suffix String to append to the user name when forming the DN to bind\n## DEPRECATED ldap.baseDN It will removed in a future, please use 'ldap.basedn' instead\n## DEPRECATED ldap.bindDN It will removed in a future, please use 'ldap.binddn' instead\n## DEPRECATED ldap.bind_password It will removed in a future, please use 'ldap.bindpw' instead\n## @param ldap.basedn Root DN to begin the search for the user in\n## @param ldap.binddn DN of user to bind to LDAP\n## @param ldap.bindpw Password for the user to bind to LDAP\n## DEPRECATED ldap.search_attr It will removed in a future, please use 'ldap.searchAttribute' instead\n## DEPRECATED ldap.search_filter It will removed in a future, please use 'ldap.searchFilter' instead\n## @param ldap.searchAttribute Attribute to match against the user name in the search\n## @param ldap.searchFilter The search filter to use when doing search+bind authentication\n## @param ldap.scheme Set to `ldaps` to use LDAPS\n## DEPRECATED ldap.tls as string is deprecatedplease use 'ldap.tls.enabled' instead\n## @param ldap.tls.enabled Se to true to enable TLS encryption\n##\nldap:\n  enabled: false\n  server: \"\"\n  port: \"\"\n  prefix: \"\"\n  suffix: \"\"\n  basedn: \"\"\n  binddn: \"\"\n  bindpw: \"\"\n  searchAttribute: \"\"\n  searchFilter: \"\"\n  scheme: \"\"\n  tls:\n    enabled: false\n  ## @param ldap.uri LDAP URL beginning in the form `ldap[s]://host[:port]/basedn`. If provided, all the other LDAP parameters will be ignored.\n  ## Ref: https://www.postgresql.org/docs/current/auth-ldap.html\n  uri: \"\"\n## @param postgresqlDataDir PostgreSQL data dir folder\n##\npostgresqlDataDir: /bitnami/postgresql/data\n## @param postgresqlSharedPreloadLibraries Shared preload libraries (comma-separated list)\n##\npostgresqlSharedPreloadLibraries: \"pgaudit\"\n## Start PostgreSQL pod(s) without limitations on shm memory.\n## By default docker and containerd (and possibly other container runtimes) limit `/dev/shm` to `64M`\n## ref: https://github.com/docker-library/postgres/issues/416\n## ref: https://github.com/containerd/containerd/issues/3654\n##\nshmVolume:\n  ## @param shmVolume.enabled Enable emptyDir volume for /dev/shm for PostgreSQL pod(s)\n  ##\n  enabled: true\n  ## @param shmVolume.sizeLimit Set this to enable a size limit on the shm tmpfs\n  ## Note: the size of the tmpfs counts against container's memory limit\n  ## e.g:\n  ## sizeLimit: 1Gi\n  ##\n  sizeLimit: \"\"\n## TLS configuration\n##\ntls:\n  ## @param tls.enabled Enable TLS traffic support\n  ##\n  enabled: false\n  ## @param tls.autoGenerated Generate automatically self-signed TLS certificates\n  ##\n  autoGenerated: false\n  ## @param tls.preferServerCiphers Whether to use the server's TLS cipher preferences rather than the client's\n  ##\n  preferServerCiphers: true\n  ## @param tls.certificatesSecret Name of an existing secret that contains the certificates\n  ##\n  certificatesSecret: \"\"\n  ## @param tls.certFilename Certificate filename\n  ##\n  certFilename: \"\"\n  ## @param tls.certKeyFilename Certificate key filename\n  ##\n  certKeyFilename: \"\"\n  ## @param tls.certCAFilename CA Certificate filename\n  ## If provided, PostgreSQL will authenticate TLS/SSL clients by requesting them a certificate\n  ## ref: https://www.postgresql.org/docs/9.6/auth-methods.html\n  ##\n  certCAFilename: \"\"\n  ## @param tls.crlFilename File containing a Certificate Revocation List\n  ##\n  crlFilename: \"\"\n\n## @section PostgreSQL Primary parameters\n##\nprimary:\n  ## @param primary.configuration PostgreSQL Primary main configuration to be injected as ConfigMap\n  ## ref: https://www.postgresql.org/docs/current/static/runtime-config.html\n  ##\n  configuration: \"\"\n  ## @param primary.pgHbaConfiguration PostgreSQL Primary client authentication configuration\n  ## ref: https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html\n  ## e.g:#\n  ## pgHbaConfiguration: |-\n  ##   local all all trust\n  ##   host all all localhost trust\n  ##   host mydatabase mysuser 192.168.0.0/24 md5\n  ##\n  pgHbaConfiguration: \"\"\n  ## @param primary.existingConfigmap Name of an existing ConfigMap with PostgreSQL Primary configuration\n  ## NOTE: `primary.configuration` and `primary.pgHbaConfiguration` will be ignored\n  ##\n  existingConfigmap: \"\"\n  ## @param primary.extendedConfiguration Extended PostgreSQL Primary configuration (appended to main or default configuration)\n  ## ref: https://github.com/bitnami/bitnami-docker-postgresql#allow-settings-to-be-loaded-from-files-other-than-the-default-postgresqlconf\n  ##\n  extendedConfiguration: \"\"\n  ## @param primary.existingExtendedConfigmap Name of an existing ConfigMap with PostgreSQL Primary extended configuration\n  ## NOTE: `primary.extendedConfiguration` will be ignored\n  ##\n  existingExtendedConfigmap: \"\"\n  ## Initdb configuration\n  ## ref: https://github.com/bitnami/bitnami-docker-postgresql/blob/master/README.md#specifying-initdb-arguments\n  ##\n  initdb:\n    ## @param primary.initdb.args PostgreSQL initdb extra arguments\n    ##\n    args: \"\"\n    ## @param primary.initdb.postgresqlWalDir Specify a custom location for the PostgreSQL transaction log\n    ##\n    postgresqlWalDir: \"\"\n    ## @param primary.initdb.scripts Dictionary of initdb scripts\n    ## Specify dictionary of scripts to be run at first boot\n    ## e.g:\n    ## scripts:\n    ##   my_init_script.sh: |\n    ##      #!/bin/sh\n    ##      echo \"Do something.\"\n    ##\n    scripts: {}\n    ## @param primary.initdb.scriptsConfigMap ConfigMap with scripts to be run at first boot\n    ## NOTE: This will override `primary.initdb.scripts`\n    ##\n    scriptsConfigMap: \"\"\n    ## @param primary.initdb.scriptsSecret Secret with scripts to be run at first boot (in case it contains sensitive information)\n    ## NOTE: This can work along `primary.initdb.scripts` or `primary.initdb.scriptsConfigMap`\n    ##\n    scriptsSecret: \"\"\n    ## @param primary.initdb.user Specify the PostgreSQL username to execute the initdb scripts\n    ##\n    user: \"\"\n    ## @param primary.initdb.password Specify the PostgreSQL password to execute the initdb scripts\n    ##\n    password: \"\"\n  ## Configure current cluster's primary server to be the standby server in other cluster.\n  ## This will allow cross cluster replication and provide cross cluster high availability.\n  ## You will need to configure pgHbaConfiguration if you want to enable this feature with local cluster replication enabled.\n  ## @param primary.standby.enabled Whether to enable current cluster's primary as standby server of another cluster or not\n  ## @param primary.standby.primaryHost The Host of replication primary in the other cluster\n  ## @param primary.standby.primaryPort The Port of replication primary in the other cluster\n  ##\n  standby:\n    enabled: false\n    primaryHost: \"\"\n    primaryPort: \"\"\n  ## @param primary.extraEnvVars Array with extra environment variables to add to PostgreSQL Primary nodes\n  ## e.g:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param primary.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for PostgreSQL Primary nodes\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param primary.extraEnvVarsSecret Name of existing Secret containing extra env vars for PostgreSQL Primary nodes\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param primary.command Override default container command (useful when using custom images)\n  ##\n  command: []\n  ## @param primary.args Override default container args (useful when using custom images)\n  ##\n  args: []\n  ## Configure extra options for PostgreSQL Primary containers' liveness, readiness and startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\n  ## @param primary.livenessProbe.enabled Enable livenessProbe on PostgreSQL Primary containers\n  ## @param primary.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param primary.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param primary.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param primary.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param primary.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param primary.readinessProbe.enabled Enable readinessProbe on PostgreSQL Primary containers\n  ## @param primary.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param primary.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param primary.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param primary.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param primary.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 5\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param primary.startupProbe.enabled Enable startupProbe on PostgreSQL Primary containers\n  ## @param primary.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param primary.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param primary.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param primary.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param primary.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: false\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 15\n    successThreshold: 1\n  ## @param primary.customLivenessProbe Custom livenessProbe that overrides the default one\n  ##\n  customLivenessProbe: {}\n  ## @param primary.customReadinessProbe Custom readinessProbe that overrides the default one\n  ##\n  customReadinessProbe: {}\n  ## @param primary.customStartupProbe Custom startupProbe that overrides the default one\n  ##\n  customStartupProbe: {}\n  ## @param primary.lifecycleHooks for the PostgreSQL Primary container to automate configuration before or after startup\n  ##\n  lifecycleHooks: {}\n  ## PostgreSQL Primary resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param primary.resources.limits The resources limits for the PostgreSQL Primary containers\n  ## @param primary.resources.requests.memory The requested memory for the PostgreSQL Primary containers\n  ## @param primary.resources.requests.cpu The requested cpu for the PostgreSQL Primary containers\n  ##\n  resources:\n    limits: {}\n    requests:\n      memory: 256Mi\n      cpu: 250m\n  ## Pod Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ## @param primary.podSecurityContext.enabled Enable security context\n  ## @param primary.podSecurityContext.fsGroup Group ID for the pod\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## Container Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ## @param primary.containerSecurityContext.enabled Enable container security context\n  ## @param primary.containerSecurityContext.runAsUser User ID for the container\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n  ## @param primary.hostAliases PostgreSQL primary pods host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## @param primary.hostNetwork Specify if host network should be enabled for PostgreSQL pod (postgresql primary)\n  ##\n  hostNetwork: false\n  ## @param primary.hostIPC Specify if host IPC should be enabled for PostgreSQL pod (postgresql primary)\n  ##\n  hostIPC: false\n  ## @param primary.labels Map of labels to add to the statefulset (postgresql primary)\n  ##\n  labels: {}\n  ## @param primary.annotations Annotations for PostgreSQL primary pods\n  ##\n  annotations: {}\n  ## @param primary.podLabels Map of labels to add to the pods (postgresql primary)\n  ##\n  podLabels: {}\n  ## @param primary.podAnnotations Map of annotations to add to the pods (postgresql primary)\n  ##\n  podAnnotations: {}\n  ## @param primary.podAffinityPreset PostgreSQL primary pod affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param primary.podAntiAffinityPreset PostgreSQL primary pod anti-affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## PostgreSQL Primary node affinity preset\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ##\n  nodeAffinityPreset:\n    ## @param primary.nodeAffinityPreset.type PostgreSQL primary node affinity preset type. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n    ##\n    type: \"\"\n    ## @param primary.nodeAffinityPreset.key PostgreSQL primary node label key to match Ignored if `primary.affinity` is set.\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## @param primary.nodeAffinityPreset.values PostgreSQL primary node label values to match. Ignored if `primary.affinity` is set.\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param primary.affinity Affinity for PostgreSQL primary pods assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ## Note: primary.podAffinityPreset, primary.podAntiAffinityPreset, and primary.nodeAffinityPreset will be ignored when it's set\n  ##\n  affinity: {}\n  ## @param primary.nodeSelector Node labels for PostgreSQL primary pods assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param primary.tolerations Tolerations for PostgreSQL primary pods assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param primary.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n  ##\n  topologySpreadConstraints: []\n  ## @param primary.priorityClassName Priority Class to use for each pod (postgresql primary)\n  ##\n  priorityClassName: \"\"\n  ## @param primary.schedulerName Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param primary.terminationGracePeriodSeconds Seconds PostgreSQL primary pod needs to terminate gracefully\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods\n  ##\n  terminationGracePeriodSeconds: \"\"\n  ## @param primary.updateStrategy.type PostgreSQL Primary statefulset strategy type\n  ## @param primary.updateStrategy.rollingUpdate PostgreSQL Primary statefulset rolling update configuration parameters\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  ##\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate: {}\n  ## @param primary.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the PostgreSQL Primary container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param primary.extraVolumes Optionally specify extra list of additional volumes for the PostgreSQL Primary pod(s)\n  ##\n  extraVolumes: []\n  ## @param primary.sidecars Add additional sidecar containers to the PostgreSQL Primary pod(s)\n  ## For example:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n  ## @param primary.initContainers Add additional init containers to the PostgreSQL Primary pod(s)\n  ## Example\n  ##\n  ## initContainers:\n  ##   - name: do-something\n  ##     image: busybox\n  ##     command: ['do', 'something']\n  ##\n  initContainers: []\n  ## @param primary.extraPodSpec Optionally specify extra PodSpec for the PostgreSQL Primary pod(s)\n  ##\n  extraPodSpec: {}\n  ## PostgreSQL Primary service configuration\n  ##\n  service:\n    ## @param primary.service.type Kubernetes Service type\n    ##\n    type: ClusterIP\n    ## @param primary.service.ports.postgresql PostgreSQL service port\n    ##\n    ports:\n      postgresql: 5432\n    ## Node ports to expose\n    ## NOTE: choose port between \u003c30000-32767\u003e\n    ## @param primary.service.nodePorts.postgresql Node port for PostgreSQL\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n    ##\n    nodePorts:\n      postgresql: \"\"\n    ## @param primary.service.clusterIP Static clusterIP or None for headless services\n    ## e.g:\n    ## clusterIP: None\n    ##\n    clusterIP: \"\"\n    ## @param primary.service.annotations Annotations for PostgreSQL primary service\n    ##\n    annotations: {}\n    ## @param primary.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`\n    ## Set the LoadBalancer service type to internal only\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n    ##\n    loadBalancerIP: \"\"\n    ## @param primary.service.externalTrafficPolicy Enable client source IP preservation\n    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    ##\n    externalTrafficPolicy: Cluster\n    ## @param primary.service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer\n    ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ##\n    ## loadBalancerSourceRanges:\n    ## - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param primary.service.extraPorts Extra ports to expose in the PostgreSQL primary service\n    ##\n    extraPorts: []\n    ## @param primary.service.sessionAffinity Session Affinity for Kubernetes service, can be \"None\" or \"ClientIP\"\n    ## If \"ClientIP\", consecutive client requests will be directed to the same Pod\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n    ##\n    sessionAffinity: None\n    ## @param primary.service.sessionAffinityConfig Additional settings for the sessionAffinity\n    ## sessionAffinityConfig:\n    ##   clientIP:\n    ##     timeoutSeconds: 300\n    ##\n    sessionAffinityConfig: {}\n  ## PostgreSQL Primary persistence configuration\n  ##\n  persistence:\n    ## @param primary.persistence.enabled Enable PostgreSQL Primary data persistence using PVC\n    ##\n    enabled: true\n    ## @param primary.persistence.existingClaim Name of an existing PVC to use\n    ##\n    existingClaim: \"\"\n    ## @param primary.persistence.mountPath The path the volume will be mounted at\n    ## Note: useful when using custom PostgreSQL images\n    ##\n    mountPath: /bitnami/postgresql\n    ## @param primary.persistence.subPath The subdirectory of the volume to mount to\n    ## Useful in dev environments and one PV for multiple services\n    ##\n    subPath: \"\"\n    ## @param primary.persistence.storageClass PVC Storage Class for PostgreSQL Primary data volume\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    storageClass: \"\"\n    ## @param primary.persistence.accessModes PVC Access Mode for PostgreSQL volume\n    ##\n    accessModes:\n      - ReadWriteOnce\n    ## @param primary.persistence.size PVC Storage Request for PostgreSQL volume\n    ##\n    size: 8Gi\n    ## @param primary.persistence.annotations Annotations for the PVC\n    ##\n    annotations: {}\n    ## @param primary.persistence.selector Selector to match an existing Persistent Volume (this value is evaluated as a template)\n    ## selector:\n    ##   matchLabels:\n    ##     app: my-app\n    ##\n    selector: {}\n    ## @param primary.persistence.dataSource Custom PVC data source\n    ##\n    dataSource: {}\n\n## @section PostgreSQL read only replica parameters\n##\nreadReplicas:\n  ## @param readReplicas.replicaCount Number of PostgreSQL read only replicas\n  ##\n  replicaCount: 1\n  ## @param readReplicas.extraEnvVars Array with extra environment variables to add to PostgreSQL read only nodes\n  ## e.g:\n  ## extraEnvVars:\n  ##   - name: FOO\n  ##     value: \"bar\"\n  ##\n  extraEnvVars: []\n  ## @param readReplicas.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for PostgreSQL read only nodes\n  ##\n  extraEnvVarsCM: \"\"\n  ## @param readReplicas.extraEnvVarsSecret Name of existing Secret containing extra env vars for PostgreSQL read only nodes\n  ##\n  extraEnvVarsSecret: \"\"\n  ## @param readReplicas.command Override default container command (useful when using custom images)\n  ##\n  command: []\n  ## @param readReplicas.args Override default container args (useful when using custom images)\n  ##\n  args: []\n  ## Configure extra options for PostgreSQL read only containers' liveness, readiness and startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\n  ## @param readReplicas.livenessProbe.enabled Enable livenessProbe on PostgreSQL read only containers\n  ## @param readReplicas.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param readReplicas.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param readReplicas.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param readReplicas.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param readReplicas.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param readReplicas.readinessProbe.enabled Enable readinessProbe on PostgreSQL read only containers\n  ## @param readReplicas.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param readReplicas.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param readReplicas.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param readReplicas.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param readReplicas.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 5\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param readReplicas.startupProbe.enabled Enable startupProbe on PostgreSQL read only containers\n  ## @param readReplicas.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param readReplicas.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param readReplicas.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param readReplicas.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param readReplicas.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: false\n    initialDelaySeconds: 30\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 15\n    successThreshold: 1\n  ## @param readReplicas.customLivenessProbe Custom livenessProbe that overrides the default one\n  ##\n  customLivenessProbe: {}\n  ## @param readReplicas.customReadinessProbe Custom readinessProbe that overrides the default one\n  ##\n  customReadinessProbe: {}\n  ## @param readReplicas.customStartupProbe Custom startupProbe that overrides the default one\n  ##\n  customStartupProbe: {}\n  ## @param readReplicas.lifecycleHooks for the PostgreSQL read only container to automate configuration before or after startup\n  ##\n  lifecycleHooks: {}\n  ## PostgreSQL read only resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param readReplicas.resources.limits The resources limits for the PostgreSQL read only containers\n  ## @param readReplicas.resources.requests.memory The requested memory for the PostgreSQL read only containers\n  ## @param readReplicas.resources.requests.cpu The requested cpu for the PostgreSQL read only containers\n  ##\n  resources:\n    limits: {}\n    requests:\n      memory: 256Mi\n      cpu: 250m\n  ## Pod Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ## @param readReplicas.podSecurityContext.enabled Enable security context\n  ## @param readReplicas.podSecurityContext.fsGroup Group ID for the pod\n  ##\n  podSecurityContext:\n    enabled: true\n    fsGroup: 1001\n  ## Container Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ## @param readReplicas.containerSecurityContext.enabled Enable container security context\n  ## @param readReplicas.containerSecurityContext.runAsUser User ID for the container\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n  ## @param readReplicas.hostAliases PostgreSQL read only pods host aliases\n  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  ##\n  hostAliases: []\n  ## @param readReplicas.hostNetwork Specify if host network should be enabled for PostgreSQL pod (PostgreSQL read only)\n  ##\n  hostNetwork: false\n  ## @param readReplicas.hostIPC Specify if host IPC should be enabled for PostgreSQL pod (postgresql primary)\n  ##\n  hostIPC: false\n  ## @param readReplicas.labels Map of labels to add to the statefulset (PostgreSQL read only)\n  ##\n  labels: {}\n  ## @param readReplicas.annotations Annotations for PostgreSQL read only pods\n  ##\n  annotations: {}\n  ## @param readReplicas.podLabels Map of labels to add to the pods (PostgreSQL read only)\n  ##\n  podLabels: {}\n  ## @param readReplicas.podAnnotations Map of annotations to add to the pods (PostgreSQL read only)\n  ##\n  podAnnotations: {}\n  ## @param readReplicas.podAffinityPreset PostgreSQL read only pod affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAffinityPreset: \"\"\n  ## @param readReplicas.podAntiAffinityPreset PostgreSQL read only pod anti-affinity preset. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\n  ##\n  podAntiAffinityPreset: soft\n  ## PostgreSQL read only node affinity preset\n  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\n  ##\n  nodeAffinityPreset:\n    ## @param readReplicas.nodeAffinityPreset.type PostgreSQL read only node affinity preset type. Ignored if `primary.affinity` is set. Allowed values: `soft` or `hard`\n    ##\n    type: \"\"\n    ## @param readReplicas.nodeAffinityPreset.key PostgreSQL read only node label key to match Ignored if `primary.affinity` is set.\n    ## E.g.\n    ## key: \"kubernetes.io/e2e-az-name\"\n    ##\n    key: \"\"\n    ## @param readReplicas.nodeAffinityPreset.values PostgreSQL read only node label values to match. Ignored if `primary.affinity` is set.\n    ## E.g.\n    ## values:\n    ##   - e2e-az1\n    ##   - e2e-az2\n    ##\n    values: []\n  ## @param readReplicas.affinity Affinity for PostgreSQL read only pods assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  ## Note: primary.podAffinityPreset, primary.podAntiAffinityPreset, and primary.nodeAffinityPreset will be ignored when it's set\n  ##\n  affinity: {}\n  ## @param readReplicas.nodeSelector Node labels for PostgreSQL read only pods assignment\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n  ## @param readReplicas.tolerations Tolerations for PostgreSQL read only pods assignment\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  ## @param readReplicas.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\n  ##\n  topologySpreadConstraints: []\n  ## @param readReplicas.priorityClassName Priority Class to use for each pod (PostgreSQL read only)\n  ##\n  priorityClassName: \"\"\n  ## @param readReplicas.schedulerName Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  schedulerName: \"\"\n  ## @param readReplicas.terminationGracePeriodSeconds Seconds PostgreSQL read only pod needs to terminate gracefully\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods\n  ##\n  terminationGracePeriodSeconds: \"\"\n  ## @param readReplicas.updateStrategy.type PostgreSQL read only statefulset strategy type\n  ## @param readReplicas.updateStrategy.rollingUpdate PostgreSQL read only statefulset rolling update configuration parameters\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  ##\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate: {}\n  ## @param readReplicas.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the PostgreSQL read only container(s)\n  ##\n  extraVolumeMounts: []\n  ## @param readReplicas.extraVolumes Optionally specify extra list of additional volumes for the PostgreSQL read only pod(s)\n  ##\n  extraVolumes: []\n  ## @param readReplicas.sidecars Add additional sidecar containers to the PostgreSQL read only pod(s)\n  ## For example:\n  ## sidecars:\n  ##   - name: your-image-name\n  ##     image: your-image\n  ##     imagePullPolicy: Always\n  ##     ports:\n  ##       - name: portname\n  ##         containerPort: 1234\n  ##\n  sidecars: []\n  ## @param readReplicas.initContainers Add additional init containers to the PostgreSQL read only pod(s)\n  ## Example\n  ##\n  ## initContainers:\n  ##   - name: do-something\n  ##     image: busybox\n  ##     command: ['do', 'something']\n  ##\n  initContainers: []\n  ## @param readReplicas.extraPodSpec Optionally specify extra PodSpec for the PostgreSQL read only pod(s)\n  ##\n  extraPodSpec: {}\n  ## PostgreSQL read only service configuration\n  ##\n  service:\n    ## @param readReplicas.service.type Kubernetes Service type\n    ##\n    type: ClusterIP\n    ## @param readReplicas.service.ports.postgresql PostgreSQL service port\n    ##\n    ports:\n      postgresql: 5432\n    ## Node ports to expose\n    ## NOTE: choose port between \u003c30000-32767\u003e\n    ## @param readReplicas.service.nodePorts.postgresql Node port for PostgreSQL\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport\n    ##\n    nodePorts:\n      postgresql: \"\"\n    ## @param readReplicas.service.clusterIP Static clusterIP or None for headless services\n    ## e.g:\n    ## clusterIP: None\n    ##\n    clusterIP: \"\"\n    ## @param readReplicas.service.annotations Annotations for PostgreSQL read only service\n    ##\n    annotations: {}\n    ## @param readReplicas.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`\n    ## Set the LoadBalancer service type to internal only\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer\n    ##\n    loadBalancerIP: \"\"\n    ## @param readReplicas.service.externalTrafficPolicy Enable client source IP preservation\n    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n    ##\n    externalTrafficPolicy: Cluster\n    ## @param readReplicas.service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer\n    ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service\n    ##\n    ## loadBalancerSourceRanges:\n    ## - 10.10.10.0/24\n    ##\n    loadBalancerSourceRanges: []\n    ## @param readReplicas.service.extraPorts Extra ports to expose in the PostgreSQL read only service\n    ##\n    extraPorts: []\n    ## @param readReplicas.service.sessionAffinity Session Affinity for Kubernetes service, can be \"None\" or \"ClientIP\"\n    ## If \"ClientIP\", consecutive client requests will be directed to the same Pod\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n    ##\n    sessionAffinity: None\n    ## @param readReplicas.service.sessionAffinityConfig Additional settings for the sessionAffinity\n    ## sessionAffinityConfig:\n    ##   clientIP:\n    ##     timeoutSeconds: 300\n    ##\n    sessionAffinityConfig: {}\n  ## PostgreSQL read only persistence configuration\n  ##\n  persistence:\n    ## @param readReplicas.persistence.enabled Enable PostgreSQL read only data persistence using PVC\n    ##\n    enabled: true\n    ## @param readReplicas.persistence.mountPath The path the volume will be mounted at\n    ## Note: useful when using custom PostgreSQL images\n    ##\n    mountPath: /bitnami/postgresql\n    ## @param readReplicas.persistence.subPath The subdirectory of the volume to mount to\n    ## Useful in dev environments and one PV for multiple services\n    ##\n    subPath: \"\"\n    ## @param readReplicas.persistence.storageClass PVC Storage Class for PostgreSQL read only data volume\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    storageClass: \"\"\n    ## @param readReplicas.persistence.accessModes PVC Access Mode for PostgreSQL volume\n    ##\n    accessModes:\n      - ReadWriteOnce\n    ## @param readReplicas.persistence.size PVC Storage Request for PostgreSQL volume\n    ##\n    size: 8Gi\n    ## @param readReplicas.persistence.annotations Annotations for the PVC\n    ##\n    annotations: {}\n    ## @param readReplicas.persistence.selector Selector to match an existing Persistent Volume (this value is evaluated as a template)\n    ## selector:\n    ##   matchLabels:\n    ##     app: my-app\n    ##\n    selector: {}\n    ## @param readReplicas.persistence.dataSource Custom PVC data source\n    ##\n    dataSource: {}\n\n## @section NetworkPolicy parameters\n\n## Add networkpolicies\n##\nnetworkPolicy:\n  ## @param networkPolicy.enabled Enable network policies\n  ##\n  enabled: false\n  ## @param networkPolicy.metrics.enabled Enable network policies for metrics (prometheus)\n  ## @param networkPolicy.metrics.namespaceSelector [object] Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.\n  ## @param networkPolicy.metrics.podSelector [object] Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.\n  ##\n  metrics:\n    enabled: false\n    ## e.g:\n    ## namespaceSelector:\n    ##   label: monitoring\n    ##\n    namespaceSelector: {}\n    ## e.g:\n    ## podSelector:\n    ##   label: monitoring\n    ##\n    podSelector: {}\n  ## Ingress Rules\n  ##\n  ingressRules:\n    ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.enabled Enable ingress rule that makes PostgreSQL primary node only accessible from a particular origin.\n    ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to access the PostgreSQL primary node. This label will be used to identified the allowed namespace(s).\n    ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.podSelector [object] Pods selector label that is allowed to access the PostgreSQL primary node. This label will be used to identified the allowed pod(s).\n    ## @param networkPolicy.ingressRules.primaryAccessOnlyFrom.customRules [object] Custom network policy for the PostgreSQL primary node.\n    ##\n    primaryAccessOnlyFrom:\n      enabled: false\n      ## e.g:\n      ## namespaceSelector:\n      ##   label: ingress\n      ##\n      namespaceSelector: {}\n      ## e.g:\n      ## podSelector:\n      ##   label: access\n      ##\n      podSelector: {}\n      ## custom ingress rules\n      ## e.g:\n      ## customRules:\n      ##   - from:\n      ##       - namespaceSelector:\n      ##           matchLabels:\n      ##             label: example\n      customRules: {}\n    ## @param networkPolicy.ingressRules.readReplicasAccessOnlyFrom.enabled Enable ingress rule that makes PostgreSQL read-only nodes only accessible from a particular origin.\n    ## @param networkPolicy.ingressRules.readReplicasAccessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to access the PostgreSQL read-only nodes. This label will be used to identified the allowed namespace(s).\n    ## @param networkPolicy.ingressRules.readReplicasAccessOnlyFrom.podSelector [object] Pods selector label that is allowed to access the PostgreSQL read-only nodes. This label will be used to identified the allowed pod(s).\n    ## @param networkPolicy.ingressRules.readReplicasAccessOnlyFrom.customRules [object] Custom network policy for the PostgreSQL read-only nodes.\n    ##\n    readReplicasAccessOnlyFrom:\n      enabled: false\n      ## e.g:\n      ## namespaceSelector:\n      ##   label: ingress\n      ##\n      namespaceSelector: {}\n      ## e.g:\n      ## podSelector:\n      ##   label: access\n      ##\n      podSelector: {}\n      ## custom ingress rules\n      ## e.g:\n      ## CustomRules:\n      ##   - from:\n      ##       - namespaceSelector:\n      ##           matchLabels:\n      ##             label: example\n      customRules: {}\n  ## @param networkPolicy.egressRules.denyConnectionsToExternal Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).\n  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule\n  ##\n  egressRules:\n    # Deny connections to external. This is not compatible with an external database.\n    denyConnectionsToExternal: false\n    ## Additional custom egress rules\n    ## e.g:\n    ## customRules:\n    ##   - to:\n    ##       - namespaceSelector:\n    ##           matchLabels:\n    ##             label: example\n    customRules: {}\n\n## @section Volume Permissions parameters\n\n## Init containers parameters:\n## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node\n##\nvolumePermissions:\n  ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume\n  ##\n  enabled: false\n  ## @param volumePermissions.image.registry Init container volume-permissions image registry\n  ## @param volumePermissions.image.repository Init container volume-permissions image repository\n  ## @param volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)\n  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy\n  ## @param volumePermissions.image.pullSecrets Init container volume-permissions image pull secrets\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/bitnami-shell\n    tag: 11-debian-11-r17\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## Init container resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param volumePermissions.resources.limits Init container volume-permissions resource limits\n  ## @param volumePermissions.resources.requests Init container volume-permissions resource requests\n  ##\n  resources:\n    limits: {}\n    requests: {}\n  ## Init container' Security Context\n  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser\n  ## and not the below volumePermissions.containerSecurityContext.runAsUser\n  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container\n  ##\n  containerSecurityContext:\n    runAsUser: 0\n\n## @section Other Parameters\n\n## Service account for PostgreSQL to use.\n## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n##\nserviceAccount:\n  ## @param serviceAccount.create Enable creation of ServiceAccount for PostgreSQL pod\n  ##\n  create: false\n  ## @param serviceAccount.name The name of the ServiceAccount to use.\n  ## If not set and create is true, a name is generated using the common.names.fullname template\n  ##\n  name: \"\"\n  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created\n  ## Can be set to false if pods using this serviceAccount do not need to use K8s API\n  ##\n  automountServiceAccountToken: true\n  ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount\n  ##\n  annotations: {}\n## Creates role for ServiceAccount\n## @param rbac.create Create Role and RoleBinding (required for PSP to work)\n##\nrbac:\n  create: false\n  ## @param rbac.rules Custom RBAC rules to set\n  ## e.g:\n  ## rules:\n  ##   - apiGroups:\n  ##       - \"\"\n  ##     resources:\n  ##       - pods\n  ##     verbs:\n  ##       - get\n  ##       - list\n  ##\n  rules: []\n## Pod Security Policy\n## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n## @param psp.create Whether to create a PodSecurityPolicy. WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later\n##\npsp:\n  create: false\n\n## @section Metrics Parameters\n\nmetrics:\n  ## @param metrics.enabled Start a prometheus exporter\n  ##\n  enabled: false\n  ## @param metrics.image.registry PostgreSQL Prometheus Exporter image registry\n  ## @param metrics.image.repository PostgreSQL Prometheus Exporter image repository\n  ## @param metrics.image.tag PostgreSQL Prometheus Exporter image tag (immutable tags are recommended)\n  ## @param metrics.image.pullPolicy PostgreSQL Prometheus Exporter image pull policy\n  ## @param metrics.image.pullSecrets Specify image pull secrets\n  ##\n  image:\n    registry: docker.io\n    repository: bitnami/postgres-exporter\n    tag: 0.10.1-debian-11-r18\n    pullPolicy: IfNotPresent\n    ## Optionally specify an array of imagePullSecrets.\n    ## Secrets must be manually created in the namespace.\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    ## Example:\n    ## pullSecrets:\n    ##   - myRegistryKeySecretName\n    ##\n    pullSecrets: []\n  ## @param metrics.customMetrics Define additional custom metrics\n  ## ref: https://github.com/wrouesnel/postgres_exporter#adding-new-metrics-via-a-config-file\n  ## customMetrics:\n  ##   pg_database:\n  ##     query: \"SELECT d.datname AS name, CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT') THEN pg_catalog.pg_database_size(d.datname) ELSE 0 END AS size_bytes FROM pg_catalog.pg_database d where datname not in ('template0', 'template1', 'postgres')\"\n  ##     metrics:\n  ##       - name:\n  ##           usage: \"LABEL\"\n  ##           description: \"Name of the database\"\n  ##       - size_bytes:\n  ##           usage: \"GAUGE\"\n  ##           description: \"Size of the database in bytes\"\n  ##\n  customMetrics: {}\n  ## @param metrics.extraEnvVars Extra environment variables to add to PostgreSQL Prometheus exporter\n  ## see: https://github.com/wrouesnel/postgres_exporter#environment-variables\n  ## For example:\n  ##  extraEnvVars:\n  ##  - name: PG_EXPORTER_DISABLE_DEFAULT_METRICS\n  ##    value: \"true\"\n  ##\n  extraEnvVars: []\n  ## PostgreSQL Prometheus exporter containers' Security Context\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\n  ## @param metrics.containerSecurityContext.enabled Enable PostgreSQL Prometheus exporter containers' Security Context\n  ## @param metrics.containerSecurityContext.runAsUser Set PostgreSQL Prometheus exporter containers' Security Context runAsUser\n  ## @param metrics.containerSecurityContext.runAsNonRoot Set PostgreSQL Prometheus exporter containers' Security Context runAsNonRoot\n  ##\n  containerSecurityContext:\n    enabled: true\n    runAsUser: 1001\n    runAsNonRoot: true\n  ## Configure extra options for PostgreSQL Prometheus exporter containers' liveness, readiness and startup probes\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\n  ## @param metrics.livenessProbe.enabled Enable livenessProbe on PostgreSQL Prometheus exporter containers\n  ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe\n  ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe\n  ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe\n  ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe\n  ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe\n  ##\n  livenessProbe:\n    enabled: true\n    initialDelaySeconds: 5\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param metrics.readinessProbe.enabled Enable readinessProbe on PostgreSQL Prometheus exporter containers\n  ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe\n  ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe\n  ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe\n  ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe\n  ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe\n  ##\n  readinessProbe:\n    enabled: true\n    initialDelaySeconds: 5\n    periodSeconds: 10\n    timeoutSeconds: 5\n    failureThreshold: 6\n    successThreshold: 1\n  ## @param metrics.startupProbe.enabled Enable startupProbe on PostgreSQL Prometheus exporter containers\n  ## @param metrics.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe\n  ## @param metrics.startupProbe.periodSeconds Period seconds for startupProbe\n  ## @param metrics.startupProbe.timeoutSeconds Timeout seconds for startupProbe\n  ## @param metrics.startupProbe.failureThreshold Failure threshold for startupProbe\n  ## @param metrics.startupProbe.successThreshold Success threshold for startupProbe\n  ##\n  startupProbe:\n    enabled: false\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    timeoutSeconds: 1\n    failureThreshold: 15\n    successThreshold: 1\n  ## @param metrics.customLivenessProbe Custom livenessProbe that overrides the default one\n  ##\n  customLivenessProbe: {}\n  ## @param metrics.customReadinessProbe Custom readinessProbe that overrides the default one\n  ##\n  customReadinessProbe: {}\n  ## @param metrics.customStartupProbe Custom startupProbe that overrides the default one\n  ##\n  customStartupProbe: {}\n  ## @param metrics.containerPorts.metrics PostgreSQL Prometheus exporter metrics container port\n  ##\n  containerPorts:\n    metrics: 9187\n  ## PostgreSQL Prometheus exporter resource requests and limits\n  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ## @param metrics.resources.limits The resources limits for the PostgreSQL Prometheus exporter container\n  ## @param metrics.resources.requests The requested resources for the PostgreSQL Prometheus exporter container\n  ##\n  resources:\n    limits: {}\n    requests: {}\n  ## Service configuration\n  ##\n  service:\n    ## @param metrics.service.ports.metrics PostgreSQL Prometheus Exporter service port\n    ##\n    ports:\n      metrics: 9187\n    ## @param metrics.service.clusterIP Static clusterIP or None for headless services\n    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\n    ##\n    clusterIP: \"\"\n    ## @param metrics.service.sessionAffinity Control where client requests go, to the same pod or round-robin\n    ## Values: ClientIP or None\n    ## ref: https://kubernetes.io/docs/user-guide/services/\n    ##\n    sessionAffinity: None\n    ## @param metrics.service.annotations [object] Annotations for Prometheus to auto-discover the metrics endpoint\n    ##\n    annotations:\n      prometheus.io/scrape: \"true\"\n      prometheus.io/port: \"{{ .Values.metrics.service.ports.metrics }}\"\n  ## Prometheus Operator ServiceMonitor configuration\n  ##\n  serviceMonitor:\n    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator\n    ##\n    enabled: false\n    ## @param metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)\n    ##\n    namespace: \"\"\n    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    interval: \"\"\n    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended\n    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint\n    ##\n    scrapeTimeout: \"\"\n    ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus\n    ##\n    labels: {}\n    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels\n    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration\n    ##\n    selector: {}\n    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping\n    ##\n    relabelings: []\n    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion\n    ##\n    metricRelabelings: []\n    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint\n    ##\n    honorLabels: false\n    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.\n    ##\n    jobLabel: \"\"\n  ## Custom PrometheusRule to be defined\n  ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart\n  ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions\n  ##\n  prometheusRule:\n    ## @param metrics.prometheusRule.enabled Create a PrometheusRule for Prometheus Operator\n    ##\n    enabled: false\n    ## @param metrics.prometheusRule.namespace Namespace for the PrometheusRule Resource (defaults to the Release Namespace)\n    ##\n    namespace: \"\"\n    ## @param metrics.prometheusRule.labels Additional labels that can be used so PrometheusRule will be discovered by Prometheus\n    ##\n    labels: {}\n    ## @param metrics.prometheusRule.rules PrometheusRule definitions\n    ## Make sure to constraint the rules to the current postgresql service.\n    ## rules:\n    ##   - alert: HugeReplicationLag\n    ##     expr: pg_replication_lag{service=\"{{ printf \"%s-metrics\" (include \"common.names.fullname\" .) }}\"} / 3600 \u003e 1\n    ##     for: 1m\n    ##     labels:\n    ##       severity: critical\n    ##     annotations:\n    ##       description: replication for {{ include \"common.names.fullname\" . }} PostgreSQL is lagging by {{ \"{{ $value }}\" }} hour(s).\n    ##       summary: PostgreSQL replication is lagging by {{ \"{{ $value }}\" }} hour(s).\n    ##\n    rules: []\n"
            ],
            "verify": false,
            "version": "11.6.19",
            "wait": true,
            "wait_for_jobs": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kind_cluster.one-click"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kind_cluster",
      "name": "one-click",
      "provider": "provider[\"registry.terraform.io/kyma-incubator/kind\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "client_certificate": "-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIQmXHoiWUagYwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE\nAxMKa3ViZXJuZXRlczAeFw0yMjA5MjEwNTIzMzFaFw0yMzA5MjEwNTIzMzNaMDQx\nFzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk\nbWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwt1ZFqiosAhVpKhi\nagjJjAJIAEjMlyDNIZcdYrqcuQl4iw9pP4BOZGojKC1nRloO70rLcOZHmRV0qcpO\nAWiB6VgmYZX8puZVuIK/nYHGCfDhWRMKH5sKdZlm14kR37n1BPP/DZokJhM8JveF\nrkKJtdIv/MWEXFjcGlBnXM2u5oIhKKCvkSDtecQgoQEgkSGAj/z4zILcCKbd2YsA\nXY3oIj0yBIuFRBmluH9bIvJJrSxVKou4LhmcZOPvRm23OBdpDMQpVlPrCBmbySJr\nvs//Oo6Dbawr8rc62j6udza9zO1Hk6UMzOsTiXxJvP5Q6j5cfM+0PIlvfspES6lx\nIREg9wIDAQABo1YwVDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBR/8l7zwPtOZLTLarVVIuLLqufh\nmTANBgkqhkiG9w0BAQsFAAOCAQEAXU3E9R3xXo62lPXXiedqURNzWb01KzHOeFHy\nLUSOs6TctWmBSchwuTAul+FFGYbiTIDvd4p/I8PkxsOsb6MB/aeSh6lBNDaTdtC9\nsIn8u3mjLXKX0bD3t5WBIN3m/jZpEpC+K4tgNnWp01KRMNodDwiTw1yntcnUc7SO\nEqVUGeQis9qTOeevf/HJkIQTAUGcW1KpYarArUnmqXBcK4Yato06Plntk6NNmqR2\nPNpidTFzI69hPyFkzZNxchwJ9XwO3xXmk0HWTUM1p3Yc/MDlGuGQa3IpqpXxiWSL\ncGdD8lacYmP+F45fY7XdA7LsLwKEdFchmHI7KIS1eHG8CjDaNQ==\n-----END CERTIFICATE-----\n",
            "client_key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEogIBAAKCAQEAwt1ZFqiosAhVpKhiagjJjAJIAEjMlyDNIZcdYrqcuQl4iw9p\nP4BOZGojKC1nRloO70rLcOZHmRV0qcpOAWiB6VgmYZX8puZVuIK/nYHGCfDhWRMK\nH5sKdZlm14kR37n1BPP/DZokJhM8JveFrkKJtdIv/MWEXFjcGlBnXM2u5oIhKKCv\nkSDtecQgoQEgkSGAj/z4zILcCKbd2YsAXY3oIj0yBIuFRBmluH9bIvJJrSxVKou4\nLhmcZOPvRm23OBdpDMQpVlPrCBmbySJrvs//Oo6Dbawr8rc62j6udza9zO1Hk6UM\nzOsTiXxJvP5Q6j5cfM+0PIlvfspES6lxIREg9wIDAQABAoIBAGjFpJHnSxOi15F5\nq7iBUwSBcX8dpUsYqtKFspoT5hIjQnnaZmdrMDsTSiXfC6kq4N7OY0YnupVXkOt9\nx6VAMKFq8PAFOdvu+mYDfs1MKlPL/Q3si8Pkk6IZQc9fdonJLTFlk1wX5ZnHO89j\nxK+mPO7SvVR2Sq1wZZ+MXZbrTD/JBXzh94hpaRLKFAbSaG/JnVX975kYeXJ3f6+f\nzfOawuhCLusNwzFJNlEYblurDO/RkxgBEiqHBksUxTxZePeLMrRaervvbmOeiiTX\nZ5egnlXH6oSehtjna5cJc1wVdIdwub5ZtPlKIHWopst9ZdP/KKL9iLTTwIRbVV3u\nRJZ7VSECgYEA58rSWFTfzv65lG4yJ1LuEzu0GHu0AqC5SCF/KSslzVYbq1JZSHMJ\nv9o744rulybYX5JwPU+4H/6Xo/l83mhG7rEVzTnPRBmFd1DGR6ghp73IKA+hsrWO\nUHBWNs4+EYeeqOZPdHURPPSb5lhleJsNUD1zV0X1Ydc8rW4MtjoNA3ECgYEA1zc7\nezADWuGSg+FzasQPT7g2ygTUQ9rA0tFEX5TbpRwreIUpihoyY2CwM1Kx+4Sd8L6A\njPgMByl9HtoTF55NNAct8t3EtL4VsCGpX3zc7QAEUZWrXObkZvwh29d9uKptlRJ0\nMXvqAxqEXYWQi8EUSbsbGIQiorCVrWhGwi00ZucCgYBisPjeo0Sn3c0mZcT0+Vpk\n1gbYZdM/Sy+Yhqqd3ME7rG4bg/R/5HtCNKlLONxdfyyhhLhFYqwC82OdbZy9Fakd\n+EhQGU0msbciFkJWQEAUK7oNVJqEfMkbdrk+nJHQ5O2o/GbYLwAn8/5tFTYfI+WH\nRKLXhfH0trAfJ7xB2nWwAQKBgERmZHwxdU+EE8ngdItBRtCCbr9kK4ZpVxFpuz5Z\nlDl0q4hBYkNnMg6No1KcfL7AlmX2Ver3xvLbx81vZ9QA1u6rPXpL64G6TDBPcoJL\nr/ePItA/Llh3da4ZgjVZ1rZDTFKtq8oyZNyZp+oFUjxooCcS1XjEPjHQx7F2S87Z\nCSePAoGAKslwCCy8TrFT/ZF5KyWiBPhabnaj6ZSvOlpZinY1rDY0tJuRY4TCNZHv\n/dJmHdeUJjHIQZb6IXdG1+3wSv7a3z5ppie1s334pJUkl9PyJI9jzi2iCXwZYmnW\nE4ets4Rmm9ajgLcQzOhBzXL0wqz+0XN9YhuwBaQNuiv32Ov7+Q0=\n-----END RSA PRIVATE KEY-----\n",
            "cluster_ca_certificate": "-----BEGIN CERTIFICATE-----\nMIIC5zCCAc+gAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl\ncm5ldGVzMB4XDTIyMDkyMTA1MjMzMVoXDTMyMDkxODA1MjMzMVowFTETMBEGA1UE\nAxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALw7\nUje3vZbBfK/1Wu0SnrFcHwpwBciJ9hFaTfXJk+hcamROD4RNCzoV4oXOgluxRYo/\nlYaCpfVnT2tU2Pi/6eS+jYi3B5dGm86cWBVHtwkxF4YR3sJGCSG57I2uzDND1nYU\nyg/LfVqNRZQTVqlyFoYruD7bM8bKQ98btzR48X8r7Eanyz77thsn0hv7mONvXz1V\nCTN541INZPy9FztYUV9lKKGzfs2/An/+Uji71WVXDXu3S71GPzDRPDuas2aFNZ+H\nHBH8ad1dLma2SlxawvJCH+VVXVTXH77DSVIcWmkQaSV7XlnRjnUYrjUNN7IYHvqm\nHBG/mjl+TTzyV0FvtpsCAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB\n/wQFMAMBAf8wHQYDVR0OBBYEFH/yXvPA+05ktMtqtVUi4suq5+GZMA0GCSqGSIb3\nDQEBCwUAA4IBAQB5Wno0oE3wBjRHGJCdu3vPSJYXhBuepAOF6NYWso2VPl4Uz4SE\n6h1plURaz/7V31O5SNG/4T2L8THl+q38i79MYCz+WCtBSYeGUsy7KYWs2tiSFlCy\nkvNsNI4KwN/hzl9AQYEpTmUKaWciVCkUisuzaiyXfS6bi15eSj7g/SEIaDBdenrO\nK28Zr2/VPuCK33y2hx73cy0sLqFiblTU96v2dUaDj3G7jwPPYoZ7qISZyDvvkNga\nRPLfAyr9hx7zoJ3BdAjKZ285Q28rg4xFSW5S6wz+2KrPGCo5OnkvsZe3ovdR/GiV\nv1qEbzZBvAfGHp6KOXaIOPNgRcLNjtPn1d1E\n-----END CERTIFICATE-----\n",
            "endpoint": "https://127.0.0.1:49916",
            "id": "one-click-",
            "kind_config": [
              {
                "api_version": "kind.x-k8s.io/v1alpha4",
                "containerd_config_patches": null,
                "kind": "Cluster",
                "networking": [],
                "node": [
                  {
                    "extra_mounts": [],
                    "extra_port_mappings": [
                      {
                        "container_port": 80,
                        "host_port": 80,
                        "listen_address": "",
                        "protocol": ""
                      },
                      {
                        "container_port": 443,
                        "host_port": 443,
                        "listen_address": "",
                        "protocol": ""
                      }
                    ],
                    "image": "",
                    "kubeadm_config_patches": [
                      "kind: InitConfiguration\nnodeRegistration:\n  kubeletExtraArgs:\n    node-labels: \"ingress-ready=true\"\n"
                    ],
                    "role": "control-plane"
                  },
                  {
                    "extra_mounts": [],
                    "extra_port_mappings": [],
                    "image": "",
                    "kubeadm_config_patches": [
                      "kind: InitConfiguration\nnodeRegistration:\n  kubeletExtraArgs:\n    node-labels: \"worker-node=true\"\n"
                    ],
                    "role": "worker"
                  },
                  {
                    "extra_mounts": [],
                    "extra_port_mappings": [],
                    "image": "",
                    "kubeadm_config_patches": [
                      "kind: InitConfiguration\nnodeRegistration:\n  kubeletExtraArgs:\n    node-labels: \"worker-node=true\"\n"
                    ],
                    "role": "worker"
                  },
                  {
                    "extra_mounts": [],
                    "extra_port_mappings": [],
                    "image": "",
                    "kubeadm_config_patches": [
                      "kind: InitConfiguration\nnodeRegistration:\n  kubeletExtraArgs:\n    node-labels: \"worker-node=true\"\n"
                    ],
                    "role": "worker"
                  }
                ]
              }
            ],
            "kubeconfig": "apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3lNVEExTWpNek1Wb1hEVE15TURreE9EQTFNak16TVZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHc3ClVqZTN2WmJCZksvMVd1MFNuckZjSHdwd0JjaUo5aEZhVGZYSmsraGNhbVJPRDRSTkN6b1Y0b1hPZ2x1eFJZby8KbFlhQ3BmVm5UMnRVMlBpLzZlUytqWWkzQjVkR204NmNXQlZIdHdreEY0WVIzc0pHQ1NHNTdJMnV6RE5EMW5ZVQp5Zy9MZlZxTlJaUVRWcWx5Rm9ZcnVEN2JNOGJLUTk4YnR6UjQ4WDhyN0Vhbnl6Nzd0aHNuMGh2N21PTnZYejFWCkNUTjU0MUlOWlB5OUZ6dFlVVjlsS0tHemZzMi9Bbi8rVWppNzFXVlhEWHUzUzcxR1B6RFJQRHVhczJhRk5aK0gKSEJIOGFkMWRMbWEyU2x4YXd2SkNIK1ZWWFZUWEg3N0RTVkljV21rUWFTVjdYbG5Sam5VWXJqVU5ON0lZSHZxbQpIQkcvbWpsK1RUenlWMEZ2dHBzQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZIL3lYdlBBKzA1a3RNdHF0VlVpNHN1cTUrR1pNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCNVdubzBvRTN3QmpSSEdKQ2R1M3ZQU0pZWGhCdWVwQU9GNk5ZV3NvMlZQbDRVejRTRQo2aDFwbFVSYXovN1YzMU81U05HLzRUMkw4VEhsK3EzOGk3OU1ZQ3orV0N0QlNZZUdVc3k3S1lXczJ0aVNGbEN5Cmt2TnNOSTRLd04vaHpsOUFRWUVwVG1VS2FXY2lWQ2tVaXN1emFpeVhmUzZiaTE1ZVNqN2cvU0VJYURCZGVuck8KSzI4WnIyL1ZQdUNLMzN5Mmh4NzNjeTBzTHFGaWJsVFU5NnYyZFVhRGozRzdqd1BQWW9aN3FJU1p5RHZ2a05nYQpSUExmQXlyOWh4N3pvSjNCZEFqS1oyODVRMjhyZzR4RlNXNVM2d3orMktyUEdDbzVPbmt2c1plM292ZFIvR2lWCnYxcUVielpCdkFmR0hwNktPWGFJT1BOZ1JjTE5qdFBuMWQxRQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://127.0.0.1:49916\n  name: kind-one-click\ncontexts:\n- context:\n    cluster: kind-one-click\n    user: kind-one-click\n  name: kind-one-click\ncurrent-context: kind-one-click\nkind: Config\npreferences: {}\nusers:\n- name: kind-one-click\n  user:\n    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJUW1YSG9pV1VhZ1l3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBNU1qRXdOVEl6TXpGYUZ3MHlNekE1TWpFd05USXpNek5hTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXd0MVpGcWlvc0FoVnBLaGkKYWdqSmpBSklBRWpNbHlETklaY2RZcnFjdVFsNGl3OXBQNEJPWkdvaktDMW5SbG9PNzByTGNPWkhtUlYwcWNwTwpBV2lCNlZnbVlaWDhwdVpWdUlLL25ZSEdDZkRoV1JNS0g1c0tkWmxtMTRrUjM3bjFCUFAvRFpva0poTThKdmVGCnJrS0p0ZEl2L01XRVhGamNHbEJuWE0ydTVvSWhLS0N2a1NEdGVjUWdvUUVna1NHQWovejR6SUxjQ0tiZDJZc0EKWFkzb0lqMHlCSXVGUkJtbHVIOWJJdkpKclN4VktvdTRMaG1jWk9QdlJtMjNPQmRwRE1RcFZsUHJDQm1ieVNKcgp2cy8vT282RGJhd3I4cmM2Mmo2dWR6YTl6TzFIazZVTXpPc1RpWHhKdlA1UTZqNWNmTSswUElsdmZzcEVTNmx4CklSRWc5d0lEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JSLzhsN3p3UHRPWkxUTGFyVlZJdUxMcXVmaAptVEFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBWFUzRTlSM3hYbzYybFBYWGllZHFVUk56V2IwMUt6SE9lRkh5CkxVU09zNlRjdFdtQlNjaHd1VEF1bCtGRkdZYmlUSUR2ZDRwL0k4UGt4c09zYjZNQi9hZVNoNmxCTkRhVGR0QzkKc0luOHUzbWpMWEtYMGJEM3Q1V0JJTjNtL2pacEVwQytLNHRnTm5XcDAxS1JNTm9kRHdpVHcxeW50Y25VYzdTTwpFcVZVR2VRaXM5cVRPZWV2Zi9ISmtJUVRBVUdjVzFLcFlhckFyVW5tcVhCY0s0WWF0bzA2UGxudGs2Tk5tcVIyClBOcGlkVEZ6STY5aFB5Rmt6Wk54Y2h3SjlYd08zeFhtazBIV1RVTTFwM1ljL01EbEd1R1FhM0lwcXBYeGlXU0wKY0dkRDhsYWNZbVArRjQ1Zlk3WGRBN0xzTHdLRWRGY2htSEk3S0lTMWVIRzhDakRhTlE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    client-key-data: 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\n",
            "kubeconfig_path": "/Users/sada/z/local/one-click/kube_local.yaml",
            "name": "one-click",
            "node_image": null,
            "timeouts": null,
            "wait_for_ready": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjMwMDAwMDAwMDAwMH19"
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_persistent_volume_claim",
      "name": "postgres-pvc",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": []
    }
  ]
}
